{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gensim\n",
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "%matplotlib  inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from rouge import Rouge \n",
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 7.15 µs\n"
     ]
    }
   ],
   "source": [
    "% time\n",
    "with open('./data_pointer_example.txt', 'r', encoding='UTF-8') as f:\n",
    "    stories = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embedding = np.load('Word2vec_pointer.npz')['E']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.strftime('%Y-%b-%d-%H-%M-%S', time.gmtime())\n",
    "\n",
    "save_model_path = os.path.join('won', ts)\n",
    "os.makedirs('./'+save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "\n",
    "  def __init__(self, vocab_file, max_size):\n",
    "    self._word_to_id = {}\n",
    "    self._id_to_word = {}\n",
    "    self._count = 0 # keeps track of total number of words in the Vocab\n",
    "\n",
    "    # [UNK], [PAD], [START] and [STOP] get the ids 0,1,2,3.\n",
    "    for w in [UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
    "      self._word_to_id[w] = self._count\n",
    "      self._id_to_word[self._count] = w\n",
    "      self._count += 1\n",
    "\n",
    "    # Read the vocab file and add words up to max_size\n",
    "    with open(vocab_file, 'r', encoding='utf-8') as vocab_f:\n",
    "      for line in vocab_f:\n",
    "        pieces = line.split()\n",
    "        if len(pieces) != 2:\n",
    "          print ('Warning: incorrectly formatted line in vocabulary file: %s\\n' % line)\n",
    "          continue\n",
    "        w = pieces[0]\n",
    "        if w in [SENTENCE_START, SENTENCE_END, UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
    "          raise Exception('<s>, </s>, [UNK], [PAD], [START] and [STOP] shouldn\\'t be in the vocab file, but %s is' % w)\n",
    "        if w in self._word_to_id:\n",
    "          raise Exception('Duplicated word in vocabulary file: %s' % w)\n",
    "        self._word_to_id[w] = self._count\n",
    "        self._id_to_word[self._count] = w\n",
    "        self._count += 1\n",
    "        if max_size != 0 and self._count >= max_size:\n",
    "          print (\"max_size of vocab was specified as %i; we now have %i words. Stopping reading.\" % (max_size, self._count))\n",
    "          break\n",
    "\n",
    "    print (\"Finished constructing vocabulary of %i total words. Last word added: %s\" % (self._count, self._id_to_word[self._count-1]))\n",
    "\n",
    "  def word2id(self, word):\n",
    "    if word not in self._word_to_id:\n",
    "      return self._word_to_id[UNKNOWN_TOKEN]\n",
    "    return self._word_to_id[word]\n",
    "\n",
    "  def id2word(self, word_id):\n",
    "    if word_id not in self._id_to_word:\n",
    "      raise ValueError('Id not found in vocab: %d' % word_id)\n",
    "    return self._id_to_word[word_id]\n",
    "\n",
    "  def size(self):\n",
    "    return self._count\n",
    "\n",
    "  def write_metadata(self, fpath):\n",
    "    print (\"Writing word embedding metadata file to %s...\" % (fpath))\n",
    "    with open(fpath, \"w\") as f:\n",
    "      fieldnames = ['word']\n",
    "      writer = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=fieldnames)\n",
    "      for i in xrange(self.size()):\n",
    "        writer.writerow({\"word\": self._id_to_word[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <s> and </s> are used in the data files to segment the abstracts into sentences. They don't receive vocab ids.\n",
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "PAD_TOKEN = '[PAD]' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNKNOWN_TOKEN = '[UNK]' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "START_DECODING = '[START]' # This has a vocab id, which is used at the start of every decoder input sequence\n",
    "STOP_DECODING = '[STOP]' # This has a vocab id, which is used at the end of untruncated target sequences\n",
    "\n",
    "# Note: none of <s>, </s>, [PAD], [UNK], [START], [STOP] should appear in the vocab file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: incorrectly formatted line in vocabulary file: 0800 555 111 356\n",
      "\n",
      "\n",
      "Warning: incorrectly formatted line in vocabulary file: 1800 333 000 139\n",
      "\n",
      "\n",
      "Warning: incorrectly formatted line in vocabulary file: 2 1/2 124\n",
      "\n",
      "\n",
      "Warning: incorrectly formatted line in vocabulary file: 3 1/2 86\n",
      "\n",
      "\n",
      "max_size of vocab was specified as 50000; we now have 50000 words. Stopping reading.\n",
      "Finished constructing vocabulary of 50000 total words. Last word added: long-delayed\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab('./vocab', 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Example(object):\n",
    "\n",
    "  def __init__(self, article, abstract_sentences, vocab):\n",
    "    # Get ids of special tokens\n",
    "    start_decoding = vocab.word2id(START_DECODING)\n",
    "    stop_decoding = vocab.word2id(STOP_DECODING)\n",
    "\n",
    "    # Process the article\n",
    "    article_words = article.split()\n",
    "    if len(article_words) > 100:\n",
    "      article_words = article_words[:100]\n",
    "    self.enc_len = len(article_words) # store the length after truncation but before padding\n",
    "    self.enc_input = [vocab.word2id(w) for w in article_words] # list of word ids; OOVs are represented by the id for UNK token\n",
    "\n",
    "    # Process the abstract\n",
    "    abstract = ' '.join(abstract_sentences) # string\n",
    "    abstract_words = abstract.split() # list of strings\n",
    "    abs_ids = [vocab.word2id(w) for w in abstract_words] # list of word ids; OOVs are represented by the id for UNK token\n",
    "\n",
    "    # Get the decoder input sequence and target sequence\n",
    "    self.dec_input, self.target = self.get_dec_inp_targ_seqs(abs_ids, 20, start_decoding, stop_decoding)\n",
    "    self.dec_len = len(self.dec_input)\n",
    "\n",
    "    # If using pointer-generator mode, we need to store some extra info\n",
    "    if True:\n",
    "      # Store a version of the enc_input where in-article OOVs are represented by their temporary OOV id; also store the in-article OOVs words themselves\n",
    "      self.enc_input_extend_vocab, self.article_oovs = article2ids(article_words, vocab)\n",
    "\n",
    "      # Get a verison of the reference summary where in-article OOVs are represented by their temporary article OOV id\n",
    "      abs_ids_extend_vocab = abstract2ids(abstract_words, vocab, self.article_oovs)\n",
    "\n",
    "      # Overwrite decoder target sequence so it uses the temp article OOV ids\n",
    "      _, self.target = self.get_dec_inp_targ_seqs(abs_ids_extend_vocab, 20, start_decoding, stop_decoding)\n",
    "\n",
    "    # Store the original strings\n",
    "    self.original_article = article\n",
    "    self.original_abstract = abstract\n",
    "    self.original_abstract_sents = abstract_sentences\n",
    "\n",
    "\n",
    "  def get_dec_inp_targ_seqs(self, sequence, max_len, start_id, stop_id):\n",
    "    inp = [start_id] + sequence[:]\n",
    "    target = sequence[:]\n",
    "    if len(inp) > max_len: # truncate\n",
    "      inp = inp[:max_len]\n",
    "      target = target[:max_len] # no end_token\n",
    "    else: # no truncation\n",
    "      target.append(stop_id) # end token\n",
    "    assert len(inp) == len(target)\n",
    "    return inp, target\n",
    "\n",
    "\n",
    "  def pad_decoder_inp_targ(self, max_len, pad_id):\n",
    "    while len(self.dec_input) < max_len:\n",
    "      self.dec_input.append(pad_id)\n",
    "    while len(self.target) < max_len:\n",
    "      self.target.append(pad_id)\n",
    "\n",
    "\n",
    "  def pad_encoder_input(self, max_len, pad_id):\n",
    "    while len(self.enc_input) < max_len:\n",
    "      self.enc_input.append(pad_id)\n",
    "    if True:\n",
    "      while len(self.enc_input_extend_vocab) < max_len:\n",
    "        self.enc_input_extend_vocab.append(pad_id)\n",
    "        \n",
    "def article2ids(article_words, vocab):\n",
    "  ids = []\n",
    "  oovs = []\n",
    "  unk_id = vocab.word2id(UNKNOWN_TOKEN)\n",
    "  for w in article_words:\n",
    "    i = vocab.word2id(w)\n",
    "    if i == unk_id: # If w is OOV\n",
    "      if w not in oovs: # Add to list of OOVs\n",
    "        oovs.append(w)\n",
    "      oov_num = oovs.index(w) # This is 0 for the first article OOV, 1 for the second article OOV...\n",
    "      ids.append(vocab.size() + oov_num) # This is e.g. 50000 for the first article OOV, 50001 for the second...\n",
    "    else:\n",
    "      ids.append(i)\n",
    "  return ids, oovs\n",
    "\n",
    "\n",
    "def abstract2ids(abstract_words, vocab, article_oovs):\n",
    "  ids = []\n",
    "  unk_id = vocab.word2id(UNKNOWN_TOKEN)\n",
    "  for w in abstract_words:\n",
    "    i = vocab.word2id(w)\n",
    "    if i == unk_id: # If w is an OOV word\n",
    "        #print(abstract_words)\n",
    "        ids.append(i) \n",
    "#       if w in article_oovs: # If w is an in-article OOV\n",
    "#         vocab_idx = vocab.size() + article_oovs.index(w) # Map to its temporary article OOV number\n",
    "#         ids.append(vocab_idx)\n",
    "#       else: # If w is an out-of-article OOV\n",
    "#         ids.append(unk_id) # Map to the UNK token id\n",
    "    else:\n",
    "        ids.append(i)\n",
    "  return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      "  0%|          | 0/288 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/288 [00:00<00:45,  6.25it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin ./chunked\\train_000.bin 598\n",
      "./chunked/train_*.bin ./chunked\\train_000.bin 611\n",
      "./chunked/train_*.bin ./chunked\\train_000.bin 676\n",
      "./chunked/train_*.bin ./chunked\\train_000.bin 687\n",
      "./chunked/train_*.bin ./chunked\\train_000.bin 986\n",
      "./chunked/train_*.bin ./chunked\\train_001.bin 276\n",
      "./chunked/train_*.bin ./chunked\\train_001.bin 339\n",
      "./chunked/train_*.bin ./chunked\\train_001.bin 561\n",
      "./chunked/train_*.bin ./chunked\\train_001.bin 640\n",
      "./chunked/train_*.bin ./chunked\\train_001.bin 825\n",
      "./chunked/train_*.bin ./chunked\\train_001.bin 903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 2/288 [00:00<00:45,  6.33it/s]\u001b[A\n",
      "  1%|          | 3/288 [00:00<00:46,  6.14it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin ./chunked\\train_002.bin 44\n",
      "./chunked/train_*.bin ./chunked\\train_002.bin 198\n",
      "./chunked/train_*.bin ./chunked\\train_002.bin 727\n",
      "./chunked/train_*.bin ./chunked\\train_002.bin 941\n",
      "./chunked/train_*.bin ./chunked\\train_002.bin 956\n",
      "./chunked/train_*.bin ./chunked\\train_003.bin 56\n",
      "./chunked/train_*.bin ./chunked\\train_003.bin 152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|▏         | 4/288 [00:00<00:47,  6.02it/s]\u001b[A\n",
      "  2%|▏         | 5/288 [00:00<00:46,  6.09it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin ./chunked\\train_003.bin 978\n",
      "./chunked/train_*.bin ./chunked\\train_004.bin 192\n",
      "./chunked/train_*.bin ./chunked\\train_004.bin 345\n",
      "./chunked/train_*.bin ./chunked\\train_004.bin 358\n",
      "./chunked/train_*.bin ./chunked\\train_004.bin 409\n",
      "./chunked/train_*.bin ./chunked\\train_004.bin 866\n",
      "./chunked/train_*.bin ./chunked\\train_004.bin 930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 6/288 [00:01<00:47,  5.88it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin ./chunked\\train_005.bin 110\n",
      "./chunked/train_*.bin ./chunked\\train_005.bin 228\n",
      "./chunked/train_*.bin ./chunked\\train_005.bin 402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 7/288 [00:01<00:48,  5.80it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin ./chunked\\train_006.bin 429\n",
      "./chunked/train_*.bin ./chunked\\train_006.bin 556\n",
      "./chunked/train_*.bin ./chunked\\train_006.bin 722\n",
      "./chunked/train_*.bin ./chunked\\train_006.bin 962\n",
      "./chunked/train_*.bin ./chunked\\train_007.bin 235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 8/288 [00:01<00:49,  5.65it/s]\u001b[A\n",
      "  3%|▎         | 9/288 [00:02<01:03,  4.39it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin ./chunked\\train_008.bin 753\n",
      "./chunked/train_*.bin ./chunked\\train_008.bin 770\n",
      "./chunked/train_*.bin ./chunked\\train_009.bin 347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 10/288 [00:02<01:01,  4.49it/s]\u001b[A\n",
      "  4%|▍         | 11/288 [00:02<01:00,  4.55it/s]\u001b[A\n",
      "  4%|▍         | 12/288 [00:02<00:59,  4.67it/s]\u001b[A\n",
      "  5%|▍         | 13/288 [00:02<00:57,  4.75it/s]\u001b[A\n",
      "  5%|▍         | 14/288 [00:02<00:56,  4.83it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin ./chunked\\train_013.bin 161\n",
      "./chunked/train_*.bin ./chunked\\train_013.bin 562\n",
      "./chunked/train_*.bin ./chunked\\train_013.bin 563\n",
      "./chunked/train_*.bin ./chunked\\train_013.bin 564\n",
      "./chunked/train_*.bin ./chunked\\train_013.bin 565\n",
      "./chunked/train_*.bin ./chunked\\train_013.bin 566\n",
      "./chunked/train_*.bin ./chunked\\train_013.bin 567\n",
      "./chunked/train_*.bin ./chunked\\train_013.bin 568\n",
      "./chunked/train_*.bin ./chunked\\train_013.bin 569\n",
      "./chunked/train_*.bin ./chunked\\train_013.bin 570\n",
      "./chunked/train_*.bin ./chunked\\train_013.bin 571\n",
      "./chunked/train_*.bin ./chunked\\train_013.bin 572\n",
      "./chunked/train_*.bin ./chunked\\train_013.bin 573\n",
      "./chunked/train_*.bin ./chunked\\train_013.bin 574\n",
      "./chunked/train_*.bin ./chunked\\train_013.bin 936\n",
      "./chunked/train_*.bin ./chunked\\train_013.bin 937\n",
      "./chunked/train_*.bin ./chunked\\train_013.bin 938\n",
      "./chunked/train_*.bin ./chunked\\train_013.bin 998\n",
      "./chunked/train_*.bin ./chunked\\train_014.bin 332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 15/288 [00:03<00:55,  4.89it/s]\u001b[A\n",
      "  6%|▌         | 16/288 [00:03<00:55,  4.94it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin ./chunked\\train_015.bin 313\n",
      "./chunked/train_*.bin ./chunked\\train_015.bin 314\n",
      "./chunked/train_*.bin ./chunked\\train_015.bin 419\n",
      "./chunked/train_*.bin ./chunked\\train_015.bin 420\n",
      "./chunked/train_*.bin ./chunked\\train_015.bin 428\n",
      "./chunked/train_*.bin ./chunked\\train_015.bin 501\n",
      "./chunked/train_*.bin ./chunked\\train_015.bin 526\n",
      "./chunked/train_*.bin ./chunked\\train_015.bin 898\n",
      "./chunked/train_*.bin ./chunked\\train_015.bin 903\n",
      "./chunked/train_*.bin ./chunked\\train_016.bin 108\n",
      "./chunked/train_*.bin ./chunked\\train_016.bin 111\n",
      "./chunked/train_*.bin ./chunked\\train_016.bin 113\n",
      "./chunked/train_*.bin ./chunked\\train_016.bin 114\n",
      "./chunked/train_*.bin ./chunked\\train_016.bin 115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|▌         | 17/288 [00:03<00:54,  4.97it/s]\u001b[A\n",
      "  6%|▋         | 18/288 [00:03<00:53,  5.02it/s]\u001b[A\n",
      "  7%|▋         | 19/288 [00:03<00:53,  5.05it/s]\u001b[A\n",
      "  7%|▋         | 20/288 [00:03<00:52,  5.10it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin ./chunked\\train_019.bin 413\n",
      "./chunked/train_*.bin ./chunked\\train_020.bin 28\n",
      "./chunked/train_*.bin ./chunked\\train_020.bin 29\n",
      "./chunked/train_*.bin ./chunked\\train_020.bin 104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  7%|▋         | 21/288 [00:04<00:52,  5.13it/s]\u001b[A\n",
      "  8%|▊         | 22/288 [00:04<00:51,  5.14it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin ./chunked\\train_021.bin 622\n",
      "./chunked/train_*.bin ./chunked\\train_021.bin 958\n",
      "./chunked/train_*.bin ./chunked\\train_022.bin 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|▊         | 23/288 [00:04<00:51,  5.17it/s]\u001b[A\n",
      "  8%|▊         | 24/288 [00:04<00:50,  5.20it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin ./chunked\\train_023.bin 376\n",
      "./chunked/train_*.bin ./chunked\\train_024.bin 14\n",
      "./chunked/train_*.bin ./chunked\\train_024.bin 314\n",
      "./chunked/train_*.bin ./chunked\\train_024.bin 350\n",
      "./chunked/train_*.bin ./chunked\\train_024.bin 363\n",
      "./chunked/train_*.bin ./chunked\\train_024.bin 395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  9%|▊         | 25/288 [00:04<00:50,  5.21it/s]\u001b[A\n",
      "  9%|▉         | 26/288 [00:04<00:49,  5.25it/s]\u001b[A\n",
      "  9%|▉         | 27/288 [00:05<00:49,  5.25it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin ./chunked\\train_026.bin 300\n",
      "./chunked/train_*.bin ./chunked\\train_027.bin 149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|▉         | 28/288 [00:05<00:49,  5.27it/s]\u001b[A\n",
      " 10%|█         | 29/288 [00:05<00:48,  5.30it/s]\u001b[A\n",
      " 10%|█         | 30/288 [00:05<00:48,  5.31it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin ./chunked\\train_029.bin 611\n",
      "./chunked/train_*.bin ./chunked\\train_030.bin 677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|█         | 31/288 [00:05<00:48,  5.31it/s]\u001b[A\n",
      " 11%|█         | 32/288 [00:05<00:47,  5.33it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin ./chunked\\train_030.bin 773\n",
      "./chunked/train_*.bin ./chunked\\train_030.bin 838\n",
      "./chunked/train_*.bin ./chunked\\train_031.bin 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|█▏        | 33/288 [00:06<00:47,  5.34it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin ./chunked\\train_032.bin 753\n",
      "./chunked/train_*.bin ./chunked\\train_033.bin 415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 12%|█▏        | 34/288 [00:06<00:47,  5.34it/s]\u001b[A\n",
      " 12%|█▏        | 35/288 [00:06<00:47,  5.36it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin ./chunked\\train_034.bin 726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 12%|█▎        | 36/288 [00:06<00:46,  5.37it/s]\u001b[A\n",
      " 13%|█▎        | 37/288 [00:06<00:46,  5.37it/s]\u001b[A\n",
      " 13%|█▎        | 38/288 [00:07<00:46,  5.39it/s]\u001b[A\n",
      " 14%|█▎        | 39/288 [00:07<00:46,  5.39it/s]\u001b[A\n",
      " 14%|█▍        | 40/288 [00:07<00:45,  5.41it/s]\u001b[A\n",
      " 14%|█▍        | 41/288 [00:07<00:45,  5.43it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin ./chunked\\train_040.bin 501\n",
      "./chunked/train_*.bin ./chunked\\train_041.bin 440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 15%|█▍        | 42/288 [00:07<00:45,  5.44it/s]\u001b[A\n",
      " 15%|█▍        | 43/288 [00:07<00:44,  5.45it/s]\u001b[A\n",
      " 15%|█▌        | 44/288 [00:08<00:44,  5.46it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin ./chunked\\train_043.bin 685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 16%|█▌        | 45/288 [00:08<00:44,  5.46it/s]\u001b[A\n",
      " 16%|█▌        | 46/288 [00:08<00:44,  5.47it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin ./chunked\\train_045.bin 40\n",
      "./chunked/train_*.bin ./chunked\\train_045.bin 369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 16%|█▋        | 47/288 [00:08<00:44,  5.47it/s]\u001b[A\n",
      " 17%|█▋        | 48/288 [00:08<00:43,  5.47it/s]\u001b[A\n",
      " 17%|█▋        | 49/288 [00:08<00:43,  5.49it/s]\u001b[A\n",
      " 17%|█▋        | 50/288 [00:09<00:43,  5.49it/s]\u001b[A\n",
      " 18%|█▊        | 51/288 [00:09<00:43,  5.49it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin ./chunked\\train_049.bin 937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 18%|█▊        | 52/288 [00:09<00:42,  5.50it/s]\u001b[A\n",
      " 18%|█▊        | 53/288 [00:09<00:42,  5.50it/s]\u001b[A\n",
      " 19%|█▉        | 54/288 [00:09<00:42,  5.50it/s]\u001b[A\n",
      " 19%|█▉        | 55/288 [00:09<00:42,  5.51it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin ./chunked\\train_053.bin 760\n",
      "./chunked/train_*.bin ./chunked\\train_054.bin 642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 19%|█▉        | 56/288 [00:10<00:42,  5.50it/s]\u001b[A\n",
      " 20%|█▉        | 57/288 [00:10<00:41,  5.50it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin ./chunked\\train_055.bin 890\n",
      "./chunked/train_*.bin ./chunked\\train_056.bin 591\n",
      "./chunked/train_*.bin ./chunked\\train_056.bin 676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 58/288 [00:10<00:41,  5.51it/s]\u001b[A\n",
      " 20%|██        | 59/288 [00:10<00:41,  5.51it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin ./chunked\\train_058.bin 341\n",
      "./chunked/train_*.bin ./chunked\\train_058.bin 447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 21%|██        | 60/288 [00:10<00:41,  5.51it/s]\u001b[A\n",
      " 21%|██        | 61/288 [00:11<00:41,  5.51it/s]\u001b[A\n",
      " 22%|██▏       | 62/288 [00:11<00:40,  5.52it/s]\u001b[A\n",
      " 22%|██▏       | 63/288 [00:11<00:40,  5.52it/s]\u001b[A\n",
      " 22%|██▏       | 64/288 [00:11<00:40,  5.52it/s]\u001b[A\n",
      " 23%|██▎       | 65/288 [00:11<00:40,  5.52it/s]\u001b[A\n",
      " 23%|██▎       | 66/288 [00:11<00:40,  5.52it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin ./chunked\\train_065.bin 483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 23%|██▎       | 67/288 [00:12<00:40,  5.52it/s]\u001b[A\n",
      " 24%|██▎       | 68/288 [00:12<00:39,  5.52it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin ./chunked\\train_067.bin 114\n",
      "./chunked/train_*.bin ./chunked\\train_067.bin 189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 24%|██▍       | 69/288 [00:12<00:39,  5.52it/s]\u001b[A\n",
      " 24%|██▍       | 70/288 [00:12<00:39,  5.52it/s]\u001b[A\n",
      " 25%|██▍       | 71/288 [00:12<00:39,  5.53it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin ./chunked\\train_069.bin 903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 25%|██▌       | 72/288 [00:13<00:39,  5.52it/s]\u001b[A\n",
      " 25%|██▌       | 73/288 [00:13<00:41,  5.23it/s]\u001b[A\n",
      " 26%|██▌       | 74/288 [00:14<00:40,  5.24it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin ./chunked\\train_072.bin 920\n",
      "./chunked/train_*.bin ./chunked\\train_072.bin 942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 26%|██▌       | 75/288 [00:14<00:40,  5.24it/s]\u001b[A\n",
      " 26%|██▋       | 76/288 [00:14<00:40,  5.24it/s]\u001b[A\n",
      " 27%|██▋       | 77/288 [00:14<00:40,  5.24it/s]\u001b[A\n",
      " 27%|██▋       | 78/288 [00:14<00:39,  5.25it/s]\u001b[A\n",
      " 27%|██▋       | 79/288 [00:15<00:39,  5.25it/s]\u001b[A\n",
      " 28%|██▊       | 80/288 [00:15<00:39,  5.26it/s]\u001b[A\n",
      " 28%|██▊       | 81/288 [00:15<00:39,  5.27it/s]\u001b[A\n",
      " 28%|██▊       | 82/288 [00:15<00:39,  5.26it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin ./chunked\\train_081.bin 686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 29%|██▉       | 83/288 [00:15<00:38,  5.27it/s]\u001b[A\n",
      " 29%|██▉       | 84/288 [00:15<00:38,  5.27it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin ./chunked\\train_083.bin 574\n",
      "./chunked/train_*.bin ./chunked\\train_083.bin 924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 30%|██▉       | 85/288 [00:16<00:38,  5.27it/s]\u001b[A\n",
      " 30%|██▉       | 86/288 [00:16<00:38,  5.28it/s]\u001b[A\n",
      " 30%|███       | 87/288 [00:16<00:38,  5.28it/s]\u001b[A\n",
      " 31%|███       | 88/288 [00:16<00:37,  5.28it/s]\u001b[A\n",
      " 31%|███       | 89/288 [00:16<00:37,  5.28it/s]\u001b[A\n",
      " 31%|███▏      | 90/288 [00:17<00:37,  5.29it/s]\u001b[A\n",
      " 32%|███▏      | 91/288 [00:17<00:37,  5.29it/s]\u001b[A\n",
      " 32%|███▏      | 92/288 [00:17<00:37,  5.29it/s]\u001b[A\n",
      " 32%|███▏      | 93/288 [00:17<00:36,  5.29it/s]\u001b[A\n",
      " 33%|███▎      | 94/288 [00:17<00:36,  5.29it/s]\u001b[A\n",
      " 33%|███▎      | 95/288 [00:17<00:36,  5.30it/s]\u001b[A\n",
      " 33%|███▎      | 96/288 [00:18<00:36,  5.29it/s]\u001b[A\n",
      " 34%|███▎      | 97/288 [00:18<00:36,  5.29it/s]\u001b[A\n",
      " 34%|███▍      | 98/288 [00:18<00:35,  5.30it/s]\u001b[A\n",
      " 34%|███▍      | 99/288 [00:18<00:35,  5.30it/s]\u001b[A\n",
      " 35%|███▍      | 100/288 [00:18<00:35,  5.30it/s]\u001b[A\n",
      " 35%|███▌      | 101/288 [00:19<00:35,  5.31it/s]\u001b[A\n",
      " 35%|███▌      | 102/288 [00:19<00:35,  5.31it/s]\u001b[A\n",
      " 36%|███▌      | 103/288 [00:19<00:34,  5.31it/s]\u001b[A\n",
      " 36%|███▌      | 104/288 [00:19<00:34,  5.31it/s]\u001b[A\n",
      " 36%|███▋      | 105/288 [00:19<00:34,  5.32it/s]\u001b[A\n",
      " 37%|███▋      | 106/288 [00:19<00:34,  5.32it/s]\u001b[A\n",
      " 37%|███▋      | 107/288 [00:20<00:34,  5.32it/s]\u001b[A\n",
      " 38%|███▊      | 108/288 [00:20<00:33,  5.32it/s]\u001b[A\n",
      " 38%|███▊      | 109/288 [00:20<00:33,  5.32it/s]\u001b[A\n",
      " 38%|███▊      | 110/288 [00:20<00:33,  5.32it/s]\u001b[A\n",
      " 39%|███▊      | 111/288 [00:20<00:33,  5.33it/s]\u001b[A\n",
      " 39%|███▉      | 112/288 [00:21<00:33,  5.33it/s]\u001b[A\n",
      " 39%|███▉      | 113/288 [00:21<00:32,  5.33it/s]\u001b[A\n",
      " 40%|███▉      | 114/288 [00:21<00:32,  5.34it/s]\u001b[A\n",
      " 40%|███▉      | 115/288 [00:21<00:32,  5.34it/s]\u001b[A\n",
      " 40%|████      | 116/288 [00:21<00:32,  5.33it/s]\u001b[A\n",
      " 41%|████      | 117/288 [00:21<00:32,  5.34it/s]\u001b[A\n",
      " 41%|████      | 118/288 [00:22<00:31,  5.34it/s]\u001b[A\n",
      " 41%|████▏     | 119/288 [00:22<00:31,  5.34it/s]\u001b[A\n",
      " 42%|████▏     | 120/288 [00:22<00:31,  5.34it/s]\u001b[A\n",
      " 42%|████▏     | 121/288 [00:22<00:31,  5.34it/s]\u001b[A\n",
      " 42%|████▏     | 122/288 [00:22<00:31,  5.35it/s]\u001b[A\n",
      " 43%|████▎     | 123/288 [00:22<00:30,  5.35it/s]\u001b[A\n",
      " 43%|████▎     | 124/288 [00:23<00:30,  5.35it/s]\u001b[A\n",
      " 43%|████▎     | 125/288 [00:23<00:30,  5.35it/s]\u001b[A\n",
      " 44%|████▍     | 126/288 [00:23<00:30,  5.35it/s]\u001b[A\n",
      " 44%|████▍     | 127/288 [00:23<00:30,  5.36it/s]\u001b[A\n",
      " 44%|████▍     | 128/288 [00:23<00:29,  5.36it/s]\u001b[A\n",
      " 45%|████▍     | 129/288 [00:24<00:29,  5.36it/s]\u001b[A\n",
      " 45%|████▌     | 130/288 [00:24<00:29,  5.36it/s]\u001b[A\n",
      " 45%|████▌     | 131/288 [00:24<00:29,  5.37it/s]\u001b[A\n",
      " 46%|████▌     | 132/288 [00:24<00:29,  5.37it/s]\u001b[A\n",
      " 46%|████▌     | 133/288 [00:24<00:28,  5.37it/s]\u001b[A\n",
      " 47%|████▋     | 134/288 [00:24<00:28,  5.38it/s]\u001b[A\n",
      " 47%|████▋     | 135/288 [00:25<00:28,  5.38it/s]\u001b[A\n",
      " 47%|████▋     | 136/288 [00:25<00:28,  5.38it/s]\u001b[A\n",
      " 48%|████▊     | 137/288 [00:25<00:28,  5.38it/s]\u001b[A\n",
      " 48%|████▊     | 138/288 [00:25<00:27,  5.38it/s]\u001b[A\n",
      " 48%|████▊     | 139/288 [00:25<00:27,  5.38it/s]\u001b[A\n",
      " 49%|████▊     | 140/288 [00:25<00:27,  5.38it/s]\u001b[A\n",
      " 49%|████▉     | 141/288 [00:26<00:27,  5.39it/s]\u001b[A\n",
      " 49%|████▉     | 142/288 [00:26<00:27,  5.39it/s]\u001b[A\n",
      " 50%|████▉     | 143/288 [00:26<00:26,  5.39it/s]\u001b[A\n",
      " 50%|█████     | 144/288 [00:26<00:26,  5.39it/s]\u001b[A\n",
      " 50%|█████     | 145/288 [00:26<00:26,  5.39it/s]\u001b[A\n",
      " 51%|█████     | 146/288 [00:27<00:26,  5.40it/s]\u001b[A\n",
      " 51%|█████     | 147/288 [00:27<00:26,  5.40it/s]\u001b[A\n",
      " 51%|█████▏    | 148/288 [00:27<00:25,  5.40it/s]\u001b[A\n",
      " 52%|█████▏    | 149/288 [00:27<00:25,  5.40it/s]\u001b[A\n",
      " 52%|█████▏    | 150/288 [00:27<00:25,  5.41it/s]\u001b[A\n",
      " 52%|█████▏    | 151/288 [00:27<00:25,  5.41it/s]\u001b[A\n",
      " 53%|█████▎    | 152/288 [00:28<00:25,  5.41it/s]\u001b[A\n",
      " 53%|█████▎    | 153/288 [00:28<00:24,  5.42it/s]\u001b[A\n",
      " 53%|█████▎    | 154/288 [00:29<00:25,  5.25it/s]\u001b[A\n",
      " 54%|█████▍    | 155/288 [00:29<00:25,  5.25it/s]\u001b[A\n",
      " 54%|█████▍    | 156/288 [00:29<00:25,  5.25it/s]\u001b[A\n",
      " 55%|█████▍    | 157/288 [00:29<00:24,  5.26it/s]\u001b[A\n",
      " 55%|█████▍    | 158/288 [00:30<00:24,  5.26it/s]\u001b[A\n",
      " 55%|█████▌    | 159/288 [00:30<00:24,  5.26it/s]\u001b[A\n",
      " 56%|█████▌    | 160/288 [00:30<00:24,  5.26it/s]\u001b[A\n",
      " 56%|█████▌    | 161/288 [00:30<00:24,  5.26it/s]\u001b[A\n",
      " 56%|█████▋    | 162/288 [00:30<00:23,  5.27it/s]\u001b[A\n",
      " 57%|█████▋    | 163/288 [00:30<00:23,  5.27it/s]\u001b[A\n",
      " 57%|█████▋    | 164/288 [00:31<00:23,  5.27it/s]\u001b[A\n",
      " 57%|█████▋    | 165/288 [00:31<00:23,  5.28it/s]\u001b[A\n",
      " 58%|█████▊    | 166/288 [00:31<00:23,  5.28it/s]\u001b[A\n",
      " 58%|█████▊    | 167/288 [00:31<00:22,  5.28it/s]\u001b[A\n",
      " 58%|█████▊    | 168/288 [00:31<00:22,  5.28it/s]\u001b[A\n",
      " 59%|█████▊    | 169/288 [00:31<00:22,  5.28it/s]\u001b[A\n",
      " 59%|█████▉    | 170/288 [00:32<00:22,  5.29it/s]\u001b[A\n",
      " 59%|█████▉    | 171/288 [00:32<00:22,  5.29it/s]\u001b[A\n",
      " 60%|█████▉    | 172/288 [00:32<00:21,  5.30it/s]\u001b[A\n",
      " 60%|██████    | 173/288 [00:32<00:21,  5.30it/s]\u001b[A\n",
      " 60%|██████    | 174/288 [00:32<00:21,  5.30it/s]\u001b[A\n",
      " 61%|██████    | 175/288 [00:32<00:21,  5.31it/s]\u001b[A\n",
      " 61%|██████    | 176/288 [00:33<00:21,  5.31it/s]\u001b[A\n",
      " 61%|██████▏   | 177/288 [00:33<00:20,  5.31it/s]\u001b[A\n",
      " 62%|██████▏   | 178/288 [00:33<00:20,  5.32it/s]\u001b[A\n",
      " 62%|██████▏   | 179/288 [00:33<00:20,  5.32it/s]\u001b[A\n",
      " 62%|██████▎   | 180/288 [00:33<00:20,  5.32it/s]\u001b[A\n",
      " 63%|██████▎   | 181/288 [00:33<00:20,  5.32it/s]\u001b[A\n",
      " 63%|██████▎   | 182/288 [00:34<00:19,  5.33it/s]\u001b[A\n",
      " 64%|██████▎   | 183/288 [00:34<00:19,  5.33it/s]\u001b[A\n",
      " 64%|██████▍   | 184/288 [00:34<00:19,  5.33it/s]\u001b[A\n",
      " 64%|██████▍   | 185/288 [00:34<00:19,  5.33it/s]\u001b[A\n",
      " 65%|██████▍   | 186/288 [00:34<00:19,  5.33it/s]\u001b[A\n",
      " 65%|██████▍   | 187/288 [00:35<00:18,  5.34it/s]\u001b[A\n",
      " 65%|██████▌   | 188/288 [00:35<00:18,  5.34it/s]\u001b[A\n",
      " 66%|██████▌   | 189/288 [00:35<00:18,  5.34it/s]\u001b[A\n",
      " 66%|██████▌   | 190/288 [00:35<00:18,  5.35it/s]\u001b[A\n",
      " 66%|██████▋   | 191/288 [00:35<00:18,  5.35it/s]\u001b[A\n",
      " 67%|██████▋   | 192/288 [00:35<00:17,  5.34it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************************************\n",
      "./chunked/train_*.bin ./chunked\\train_191.bin 183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████▋   | 193/288 [00:36<00:17,  5.35it/s]\u001b[A\n",
      " 67%|██████▋   | 194/288 [00:36<00:17,  5.35it/s]\u001b[A\n",
      " 68%|██████▊   | 195/288 [00:36<00:17,  5.35it/s]\u001b[A\n",
      " 68%|██████▊   | 196/288 [00:36<00:17,  5.35it/s]\u001b[A\n",
      " 68%|██████▊   | 197/288 [00:36<00:16,  5.35it/s]\u001b[A\n",
      " 69%|██████▉   | 198/288 [00:36<00:16,  5.36it/s]\u001b[A\n",
      " 69%|██████▉   | 199/288 [00:37<00:16,  5.36it/s]\u001b[A\n",
      " 69%|██████▉   | 200/288 [00:37<00:16,  5.36it/s]\u001b[A\n",
      " 70%|██████▉   | 201/288 [00:37<00:16,  5.37it/s]\u001b[A\n",
      " 70%|███████   | 202/288 [00:37<00:16,  5.37it/s]\u001b[A\n",
      " 70%|███████   | 203/288 [00:37<00:15,  5.37it/s]\u001b[A\n",
      " 71%|███████   | 204/288 [00:38<00:15,  5.37it/s]\u001b[A\n",
      " 71%|███████   | 205/288 [00:38<00:15,  5.37it/s]\u001b[A\n",
      " 72%|███████▏  | 206/288 [00:38<00:15,  5.37it/s]\u001b[A\n",
      " 72%|███████▏  | 207/288 [00:38<00:15,  5.38it/s]\u001b[A\n",
      " 72%|███████▏  | 208/288 [00:38<00:14,  5.38it/s]\u001b[A\n",
      " 73%|███████▎  | 209/288 [00:38<00:14,  5.38it/s]\u001b[A\n",
      " 73%|███████▎  | 210/288 [00:39<00:14,  5.38it/s]\u001b[A\n",
      " 73%|███████▎  | 211/288 [00:39<00:14,  5.39it/s]\u001b[A\n",
      " 74%|███████▎  | 212/288 [00:39<00:14,  5.39it/s]\u001b[A\n",
      " 74%|███████▍  | 213/288 [00:39<00:13,  5.39it/s]\u001b[A\n",
      " 74%|███████▍  | 214/288 [00:39<00:13,  5.39it/s]\u001b[A\n",
      " 75%|███████▍  | 215/288 [00:39<00:13,  5.39it/s]\u001b[A\n",
      " 75%|███████▌  | 216/288 [00:40<00:13,  5.39it/s]\u001b[A\n",
      " 75%|███████▌  | 217/288 [00:40<00:13,  5.39it/s]\u001b[A\n",
      " 76%|███████▌  | 218/288 [00:40<00:12,  5.39it/s]\u001b[A\n",
      " 76%|███████▌  | 219/288 [00:40<00:12,  5.40it/s]\u001b[A\n",
      " 76%|███████▋  | 220/288 [00:40<00:12,  5.40it/s]\u001b[A\n",
      " 77%|███████▋  | 221/288 [00:40<00:12,  5.40it/s]\u001b[A\n",
      " 77%|███████▋  | 222/288 [00:41<00:12,  5.39it/s]\u001b[A\n",
      " 77%|███████▋  | 223/288 [00:41<00:12,  5.40it/s]\u001b[A\n",
      " 78%|███████▊  | 224/288 [00:41<00:11,  5.40it/s]\u001b[A\n",
      " 78%|███████▊  | 225/288 [00:41<00:11,  5.40it/s]\u001b[A\n",
      " 78%|███████▊  | 226/288 [00:41<00:11,  5.40it/s]\u001b[A\n",
      " 79%|███████▉  | 227/288 [00:42<00:11,  5.40it/s]\u001b[A\n",
      " 79%|███████▉  | 228/288 [00:42<00:11,  5.40it/s]\u001b[A\n",
      " 80%|███████▉  | 229/288 [00:42<00:10,  5.40it/s]\u001b[A\n",
      " 80%|███████▉  | 230/288 [00:42<00:10,  5.40it/s]\u001b[A\n",
      " 80%|████████  | 231/288 [00:42<00:10,  5.40it/s]\u001b[A\n",
      " 81%|████████  | 232/288 [00:42<00:10,  5.40it/s]\u001b[A\n",
      " 81%|████████  | 233/288 [00:43<00:10,  5.40it/s]\u001b[A\n",
      " 81%|████████▏ | 234/288 [00:43<00:09,  5.40it/s]\u001b[A\n",
      " 82%|████████▏ | 235/288 [00:43<00:09,  5.40it/s]\u001b[A\n",
      " 82%|████████▏ | 236/288 [00:43<00:09,  5.40it/s]\u001b[A\n",
      " 82%|████████▏ | 237/288 [00:43<00:09,  5.40it/s]\u001b[A\n",
      " 83%|████████▎ | 238/288 [00:44<00:09,  5.40it/s]\u001b[A\n",
      " 83%|████████▎ | 239/288 [00:44<00:09,  5.40it/s]\u001b[A\n",
      " 83%|████████▎ | 240/288 [00:44<00:08,  5.40it/s]\u001b[A\n",
      " 84%|████████▎ | 241/288 [00:44<00:08,  5.40it/s]\u001b[A\n",
      " 84%|████████▍ | 242/288 [00:44<00:08,  5.41it/s]\u001b[A\n",
      " 84%|████████▍ | 243/288 [00:44<00:08,  5.41it/s]\u001b[A\n",
      " 85%|████████▍ | 244/288 [00:45<00:08,  5.41it/s]\u001b[A\n",
      " 85%|████████▌ | 245/288 [00:45<00:07,  5.41it/s]\u001b[A\n",
      " 85%|████████▌ | 246/288 [00:45<00:07,  5.41it/s]\u001b[A\n",
      " 86%|████████▌ | 247/288 [00:45<00:07,  5.41it/s]\u001b[A\n",
      " 86%|████████▌ | 248/288 [00:45<00:07,  5.41it/s]\u001b[A\n",
      " 86%|████████▋ | 249/288 [00:46<00:07,  5.41it/s]\u001b[A\n",
      " 87%|████████▋ | 250/288 [00:46<00:07,  5.41it/s]\u001b[A\n",
      " 87%|████████▋ | 251/288 [00:46<00:06,  5.41it/s]\u001b[A\n",
      " 88%|████████▊ | 252/288 [00:46<00:06,  5.41it/s]\u001b[A\n",
      " 88%|████████▊ | 253/288 [00:46<00:06,  5.41it/s]\u001b[A\n",
      " 88%|████████▊ | 254/288 [00:46<00:06,  5.41it/s]\u001b[A\n",
      " 89%|████████▊ | 255/288 [00:48<00:06,  5.27it/s]\u001b[A\n",
      " 89%|████████▉ | 256/288 [00:48<00:06,  5.27it/s]\u001b[A\n",
      " 89%|████████▉ | 257/288 [00:48<00:05,  5.27it/s]\u001b[A\n",
      " 90%|████████▉ | 258/288 [00:48<00:05,  5.27it/s]\u001b[A\n",
      " 90%|████████▉ | 259/288 [00:49<00:05,  5.27it/s]\u001b[A\n",
      " 90%|█████████ | 260/288 [00:49<00:05,  5.27it/s]\u001b[A\n",
      " 91%|█████████ | 261/288 [00:49<00:05,  5.27it/s]\u001b[A\n",
      " 91%|█████████ | 262/288 [00:49<00:04,  5.27it/s]\u001b[A\n",
      " 91%|█████████▏| 263/288 [00:49<00:04,  5.28it/s]\u001b[A\n",
      " 92%|█████████▏| 264/288 [00:50<00:04,  5.28it/s]\u001b[A\n",
      " 92%|█████████▏| 265/288 [00:50<00:04,  5.28it/s]\u001b[A\n",
      " 92%|█████████▏| 266/288 [00:50<00:04,  5.28it/s]\u001b[A\n",
      " 93%|█████████▎| 267/288 [00:50<00:03,  5.28it/s]\u001b[A\n",
      " 93%|█████████▎| 268/288 [00:50<00:03,  5.28it/s]\u001b[A\n",
      " 93%|█████████▎| 269/288 [00:50<00:03,  5.28it/s]\u001b[A\n",
      " 94%|█████████▍| 270/288 [00:51<00:03,  5.28it/s]\u001b[A\n",
      " 94%|█████████▍| 271/288 [00:51<00:03,  5.29it/s]\u001b[A\n",
      " 94%|█████████▍| 272/288 [00:51<00:03,  5.28it/s]\u001b[A\n",
      " 95%|█████████▍| 273/288 [00:51<00:02,  5.29it/s]\u001b[A\n",
      " 95%|█████████▌| 274/288 [00:51<00:02,  5.29it/s]\u001b[A\n",
      " 95%|█████████▌| 275/288 [00:52<00:02,  5.29it/s]\u001b[A\n",
      " 96%|█████████▌| 276/288 [00:52<00:02,  5.29it/s]\u001b[A\n",
      " 96%|█████████▌| 277/288 [00:52<00:02,  5.29it/s]\u001b[A\n",
      " 97%|█████████▋| 278/288 [00:52<00:01,  5.29it/s]\u001b[A\n",
      " 97%|█████████▋| 279/288 [00:52<00:01,  5.29it/s]\u001b[A\n",
      " 97%|█████████▋| 280/288 [00:52<00:01,  5.29it/s]\u001b[A\n",
      " 98%|█████████▊| 281/288 [00:53<00:01,  5.29it/s]\u001b[A\n",
      " 98%|█████████▊| 282/288 [00:53<00:01,  5.29it/s]\u001b[A\n",
      " 98%|█████████▊| 283/288 [00:53<00:00,  5.29it/s]\u001b[A\n",
      " 99%|█████████▊| 284/288 [00:53<00:00,  5.29it/s]\u001b[A\n",
      " 99%|█████████▉| 285/288 [00:53<00:00,  5.29it/s]\u001b[A\n",
      " 99%|█████████▉| 286/288 [00:54<00:00,  5.29it/s]\u001b[A\n",
      "100%|█████████▉| 287/288 [00:54<00:00,  5.29it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [00:54<01:48, 54.28s/it]t/s]\u001b[A\n",
      "  0%|          | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|▋         | 1/14 [00:00<00:02,  5.85it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "287112\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 14%|█▍        | 2/14 [00:00<00:02,  5.89it/s]\u001b[A\n",
      " 21%|██▏       | 3/14 [00:00<00:02,  5.45it/s]\u001b[A\n",
      " 29%|██▊       | 4/14 [00:00<00:01,  5.54it/s]\u001b[A\n",
      " 36%|███▌      | 5/14 [00:00<00:01,  5.61it/s]\u001b[A\n",
      " 43%|████▎     | 6/14 [00:01<00:01,  5.65it/s]\u001b[A\n",
      " 50%|█████     | 7/14 [00:01<00:01,  5.56it/s]\u001b[A\n",
      " 57%|█████▋    | 8/14 [00:01<00:01,  5.39it/s]\u001b[A\n",
      " 64%|██████▍   | 9/14 [00:01<00:00,  5.43it/s]\u001b[A\n",
      " 71%|███████▏  | 10/14 [00:01<00:00,  5.43it/s]\u001b[A\n",
      " 79%|███████▊  | 11/14 [00:02<00:00,  5.42it/s]\u001b[A\n",
      " 86%|████████▌ | 12/14 [00:02<00:00,  5.41it/s]\u001b[A\n",
      " 93%|█████████▎| 13/14 [00:02<00:00,  5.44it/s]\u001b[A\n",
      " 67%|██████▋   | 2/3 [00:56<00:28, 28.38s/it]s]\u001b[A\n",
      "  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|▊         | 1/12 [00:00<00:01,  6.27it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "13368\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 17%|█▋        | 2/12 [00:00<00:01,  5.98it/s]\u001b[A\n",
      " 25%|██▌       | 3/12 [00:00<00:01,  6.13it/s]\u001b[A\n",
      " 33%|███▎      | 4/12 [00:00<00:01,  6.21it/s]\u001b[A\n",
      " 42%|████▏     | 5/12 [00:00<00:01,  6.14it/s]\u001b[A\n",
      " 50%|█████     | 6/12 [00:00<00:00,  6.11it/s]\u001b[A\n",
      " 58%|█████▊    | 7/12 [00:01<00:00,  6.07it/s]\u001b[A\n",
      " 67%|██████▋   | 8/12 [00:01<00:00,  6.02it/s]\u001b[A\n",
      " 75%|███████▌  | 9/12 [00:01<00:00,  6.01it/s]\u001b[A\n",
      " 83%|████████▎ | 10/12 [00:01<00:00,  5.99it/s]\u001b[A\n",
      " 92%|█████████▏| 11/12 [00:01<00:00,  5.95it/s]\u001b[A\n",
      "100%|██████████| 3/3 [00:58<00:00, 19.56s/it]s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "11490\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_decoding = vocab.word2id(START_DECODING)\n",
    "stop_decoding = vocab.word2id(STOP_DECODING)\n",
    "\n",
    "input_index=[]\n",
    "dec_inputs_index=[]\n",
    "target_index=[]\n",
    "inputs_len=[]\n",
    "dec_inputs_len=[]\n",
    "\n",
    "index_data = {}\n",
    "\n",
    "zero_len = 0\n",
    "\n",
    "dec_max_len = 20\n",
    "enc_max_len = 100\n",
    "\n",
    "pad_id = vocab.word2id(PAD_TOKEN)\n",
    "files_group = list(stories.keys())\n",
    "\n",
    "for file_group in tqdm.tqdm(files_group):\n",
    "    for file in tqdm.tqdm(list(stories[file_group].keys())):       \n",
    "        for data in list(stories[file_group][file].keys()):\n",
    "            if stories[file_group][file][data]['abstract_sentences'] == []:\n",
    "                print('*********************************************')\n",
    "                abstract = stories[file_group][file][data]['abstract']\n",
    "                abstract_sentences = []\n",
    "                article = stories[file_group][file][data]['article']\n",
    "                example=Example(article=article,abstract_sentences=abstract_sentences,vocab=vocab)\n",
    "                example.pad_decoder_inp_targ(dec_max_len,pad_id)\n",
    "                example.pad_encoder_input(enc_max_len,pad_id)\n",
    "            else:\n",
    "                abstract = stories[file_group][file][data]['abstract']\n",
    "                abstract_sentences = [stories[file_group][file][data]['abstract_sentences'][0]]\n",
    "                article = stories[file_group][file][data]['article']\n",
    "                example=Example(article=article,abstract_sentences=abstract_sentences,vocab=vocab)\n",
    "                example.pad_decoder_inp_targ(dec_max_len,pad_id)\n",
    "                example.pad_encoder_input(enc_max_len,pad_id)\n",
    "            if example.enc_len <= 0 or example.dec_len <= 0:\n",
    "                print(file_group, file, data)\n",
    "                zero_len +=1\n",
    "                pass\n",
    "            else:\n",
    "                input_index.append(example.enc_input)\n",
    "                dec_inputs_index.append(example.dec_input)\n",
    "                target_index.append(example.target)\n",
    "                inputs_len.append(example.enc_len)\n",
    "                dec_inputs_len.append(example.dec_len)\n",
    "    print('********************************************************************************')\n",
    "    print(len(input_index))\n",
    "    print('********************************************************************************')\n",
    "\n",
    "    index_data[file_group] = (input_index,inputs_len,dec_inputs_index,dec_inputs_len,target_index)\n",
    "    input_index=[]\n",
    "    dec_inputs_index=[]\n",
    "    target_index=[]\n",
    "    inputs_len=[]\n",
    "    dec_inputs_len=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin\n",
      "287112\n",
      "86133\n",
      "./chunked/val_*.bin\n",
      "13368\n",
      "4010\n",
      "./chunked/test_*.bin\n",
      "11490\n",
      "3447\n"
     ]
    }
   ],
   "source": [
    "for file_group in files_group:\n",
    "    print(file_group)\n",
    "    (input_index,inputs_len,dec_inputs_index,dec_inputs_len,target_index) = index_data[file_group]\n",
    "    print(len(input_index))\n",
    "    start = int(len(input_index)*0.7)\n",
    "    end = -1\n",
    "    index_data[file_group] = (input_index[start:end],inputs_len[start:end],dec_inputs_index[start:end],dec_inputs_len[start:end],target_index[start:end])\n",
    "    (input_index,inputs_len,dec_inputs_index,dec_inputs_len,target_index) = index_data[file_group]\n",
    "    print(len(input_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "hidden_size = 256\n",
    "bidirectional = True\n",
    "batch_size_fit = 1024-256\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "vocab_size = len(vocab._word_to_id)\n",
    "\n",
    "# <s> and </s> are used in the data files to segment the abstracts into sentences. They don't receive vocab ids.\n",
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "PAD_TOKEN = '[PAD]' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNKNOWN_TOKEN = '[UNK]' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "START_DECODING = '[START]' # This has a vocab id, which is used at the start of every decoder input sequence\n",
    "STOP_DECODING = '[STOP]' # This has a vocab id, which is used at the end of untruncated target sequences\n",
    "\n",
    "# Note: none of <s>, </s>, [PAD], [UNK], [START], [STOP] should appear in the vocab file.\n",
    "\n",
    "sos_idx = vocab.word2id(START_DECODING)\n",
    "eos_idx = vocab.word2id(STOP_DECODING)\n",
    "pad_idx = vocab.word2id(PAD_TOKEN)\n",
    "unk_idx = vocab.word2id(UNKNOWN_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size,numpy_embedding):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.model_embedding = torch.from_numpy(numpy_embedding).float()\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        #self.embed.weight = nn.Parameter(self.model_embedding)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=embed_size,  #num_layers = 2,\n",
    "            hidden_size=hidden_size, batch_first=True, \n",
    "            bidirectional=True,)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x)\n",
    "        # input: [b x seq]\n",
    "        embedded = self.embed(x)\n",
    "        #print(embedded)\n",
    "        out, h = self.lstm(embedded) # out: [b x seq x hid*2] (biRNN)\n",
    "        return out, h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hypothesis(object):\n",
    "    def __init__(self, token_id, hidden_state, cell_state, log_prob):\n",
    "        self._h = hidden_state\n",
    "        self._c = cell_state\n",
    "        self.log_prob = log_prob\n",
    "        self.full_prediction = token_id # list\n",
    "        self.survivability = self.log_prob/ float(len(self.full_prediction))\n",
    "\n",
    "    def extend(self, token_id, hidden_state, cell_state, log_prob):\n",
    "        return Hypothesis(token_id= self.full_prediction + [token_id],\n",
    "                          hidden_state=hidden_state,\n",
    "                          cell_state=cell_state,\n",
    "                          log_prob= self.log_prob + log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pointer_Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size,numpy_embedding,lamda = 1):\n",
    "        super(Pointer_Decoder, self).__init__()\n",
    "        self.lamda = lamda\n",
    "        self.model_embedding = torch.from_numpy(numpy_embedding).float()\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        #self.embed.weight = nn.Parameter(self.model_embedding)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=embed_size,\n",
    "            hidden_size=hidden_size*2, batch_first=True)\n",
    "        \n",
    "        self.V2V = nn.Linear(hidden_size * 4, hidden_size * 4)\n",
    "        self.V = nn.Linear(hidden_size * 4, vocab_size)\n",
    "        \n",
    "        self.Wh = nn.Linear(2 * hidden_size, 2* hidden_size)\n",
    "        self.Ws = nn.Linear(hidden_size*2, 2*hidden_size)\n",
    "        self.w_c = nn.Linear(1, 2*hidden_size)\n",
    "        self.v = nn.Linear(2*hidden_size, 1)\n",
    "\n",
    "        self.w_h = nn.Linear(hidden_size*2, 1)\n",
    "        self.w_s = nn.Linear(hidden_size*2, 1)\n",
    "        self.w_x = nn.Linear(embedding_size, 1)\n",
    "        \n",
    "    def forward(self, x, encoder_outputs, encoder_hidden, encoder_cell_state,encoder_inputs, coverage):\n",
    "        #print(x)\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        enc_max_len = encoder_outputs.size(1)\n",
    "\n",
    "        embedded = self.embed(x).view(encoder_outputs.size(0),1, -1)\n",
    "        self.state = (encoder_hidden.view(-1,batch_size,hidden_size*2), encoder_cell_state.view(-1,batch_size,hidden_size*2))\n",
    "\n",
    "        enc_proj=self.Wh(encoder_outputs.contiguous().view(batch_size,enc_max_len,-1)) # batch_size x enc_max_len x hidden x 2\n",
    "        \n",
    "        decoder_outputs, (decoder_hidden, decoder_cell_state) = self.lstm(embedded, self.state) # out: [b x seq x hid*2] (biRNN)\n",
    "        \n",
    "        dec_proj = self.Ws(decoder_outputs).expand_as(enc_proj) # batch_size x enc_max_len x hidden x 2\n",
    "\n",
    "        cov_proj = self.w_c(coverage.view(-1,1)).view(batch_size,enc_max_len,-1) # batch_size x enc_max_len x hidden x 2\n",
    "\n",
    "        attn_scores = self.v(F.tanh(enc_proj + dec_proj + cov_proj)).view(batch_size, enc_max_len)# batch_size x enc_max_len\n",
    "        del enc_proj\n",
    "        del dec_proj\n",
    "        del cov_proj\n",
    "        #enc_mask = encoder_inputs.eq_(pad_idx).clone().type(torch.cuda.ByteTensor) # pad 버림\n",
    "\n",
    "        #attn_scores = attn_scores.masked_fill_(enc_mask, -float(10000)) # pad 버림\n",
    "        \n",
    "        attn_scores = F.softmax(attn_scores)\n",
    "\n",
    "        context = attn_scores.unsqueeze(1).bmm(encoder_outputs) #batch x 1 x hidden x 2\n",
    "\n",
    "        p_vocab = F.softmax(self.V(self.V2V(torch.cat((decoder_outputs, context), 2)))) #batch x 1 x vocab\n",
    "        #p_vocab = F.softmax(self.V(torch.cat((decoder_outputs, context), 2))) #batch x 1 x vocab\n",
    "        p_gen = F.sigmoid(self.w_h(context).squeeze(2) + self.w_s(decoder_outputs).squeeze(2) + self.w_x(embedded).squeeze(2)) #batch x 1\n",
    "\n",
    "        weighted_Pvocab = p_vocab.squeeze(1) * p_gen #batch x vocab\n",
    "        del p_vocab\n",
    "        weighted_attn  = (1-p_gen)*attn_scores #batch x vocab\n",
    "        \n",
    "#         구현예정\n",
    "#         if self.max_article_oov > 0:\n",
    "#             ext_vocab = Variable(torch.zeros(batch_size, self.max_article_oov).cuda())\t\t\t\t#create OOV (but in-article) zero vectors\n",
    "#             combined_vocab = torch.cat((weighted_Pvocab, ext_vocab), 1)\t\t\t\n",
    "#             del ext_vocab\n",
    "#         else:\n",
    "#             combined_vocab = weighted_Pvocab\n",
    "\n",
    "        combined_vocab  = weighted_Pvocab #batch x vocab\n",
    "    \n",
    "        enc_mask = encoder_inputs.clone()\n",
    "        \n",
    "        combined_vocab =combined_vocab.scatter_add(1, enc_mask, weighted_attn) #batch x vocab\n",
    "        del enc_mask\n",
    "        \n",
    "        return combined_vocab, decoder_hidden, decoder_cell_state, attn_scores, p_gen, attn_scores\n",
    "        \n",
    "        \n",
    "    def decode_step(self, x, encoder_outputs, encoder_hidden, encoder_cell_state, encoder_inputs):\n",
    "        #print(x)\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        enc_max_len = encoder_outputs.size(1)\n",
    "        coverage =  Variable(torch.zeros(batch_size, enc_max_len)).cuda()\n",
    "        \n",
    "        embedded = self.embed(x).view(encoder_outputs.size(0),1, -1)\n",
    "        state = (encoder_hidden.view(-1,batch_size,hidden_size*2), encoder_cell_state.view(-1,batch_size,hidden_size*2))\n",
    "\n",
    "        enc_proj=self.Wh(encoder_outputs.contiguous().view(batch_size,enc_max_len,-1)) # batch_size x enc_max_len x hidden x 2\n",
    "\n",
    "        decoder_outputs, (decoder_hidden, decoder_cell_state) = self.lstm(embedded, state) # out: [b x seq x hid*2] (biRNN)\n",
    "\n",
    "        dec_proj = self.Ws(decoder_outputs).expand_as(enc_proj) # batch_size x enc_max_len x hidden x 2\n",
    "\n",
    "        cov_proj = self.w_c(coverage.view(-1,1)).view(batch_size,enc_max_len,-1) # batch_size x enc_max_len x hidden x 2\n",
    "\n",
    "        attn_scores = self.v(F.tanh(enc_proj + dec_proj + cov_proj)).view(batch_size, enc_max_len)# batch_size x enc_max_len\n",
    "        #print(enc_proj.size(),dec_proj.size(),cov_proj.size())\n",
    "        enc_mask = encoder_inputs.eq_(pad_idx).detach().type(torch.cuda.ByteTensor) # pad 버림\n",
    "\n",
    "        attn_scores = attn_scores.masked_fill_(enc_mask, -float(10000)) # pad 버림\n",
    "\n",
    "        attn_scores = F.softmax(attn_scores)\n",
    "\n",
    "        context = attn_scores.unsqueeze(1).bmm(encoder_outputs) #batch x 1 x hidden x 2\n",
    "\n",
    "        p_vocab = F.softmax(self.V(torch.cat((decoder_outputs, context), 2))) #batch x 1 x vocab\n",
    "\n",
    "        p_gen = F.sigmoid(self.w_h(context).squeeze(2) + self.w_s(decoder_outputs).squeeze(2) + self.w_x(embedded).squeeze(2)) #batch x 1\n",
    "\n",
    "        weighted_Pvocab = p_vocab.squeeze(1) * p_gen #batch x vocab\n",
    "\n",
    "        weighted_attn  = (1-p_gen)*attn_scores #batch x vocab\n",
    "\n",
    "        combined_vocab  = weighted_Pvocab #batch x vocab\n",
    "\n",
    "        combined_vocab =combined_vocab.scatter_add(1, encoder_inputs, weighted_attn) #batch x vocab\n",
    "\n",
    "\n",
    "        return combined_vocab, decoder_hidden, decoder_cell_state\n",
    "\n",
    "    def getOverallTopk(self, vocab_probs, _h, _c, all_hyps, results):\n",
    "        # return top-k values i.e. top-k over all beams i.e. next step input ids\n",
    "        # return hidden, cell states corresponding to topk\n",
    "        probs, inds = vocab_probs.topk(k=2, dim=1)\n",
    "        probs = probs.log().data\n",
    "        inds = inds.data\n",
    "        #inds.add_(1)\n",
    "        candidates = []\n",
    "\n",
    "        assert len(all_hyps) == probs.size(0), '# Hypothesis and log-prob size dont match'\n",
    "        # cycle through all hypothesis in full beam\n",
    "        for i, hypo in enumerate(probs.tolist()):\n",
    "            for j, _ in enumerate(hypo):\n",
    "                new_cand = all_hyps[i].extend(token_id=inds[i,j],\n",
    "                                              hidden_state=_h[i].unsqueeze(0),\n",
    "                                              cell_state=_c[i].unsqueeze(0),\n",
    "                                              log_prob= probs[i,j])\n",
    "                candidates.append(new_cand)\n",
    "        # sort in descending order\n",
    "        candidates = sorted(candidates, key=lambda x:x.survivability, reverse=True)\n",
    "        new_beam, next_inp = [], []\n",
    "        next_h, next_c = [], []\n",
    "        #prune hypotheses and generate new beam\n",
    "        for h in candidates:\n",
    "            if h.full_prediction[-1] == eos_idx:\n",
    "                # weed out small sentences that likely have no meaning\n",
    "                if len(h.full_prediction)>=5:\n",
    "                    results.append(h.full_prediction)\n",
    "            else:\n",
    "                new_beam.append(h)\n",
    "                next_inp.append(h.full_prediction[-1])\n",
    "                next_h.append(h._h.data)\n",
    "                next_c.append(h._c.data)\n",
    "            if len(new_beam) >= 2:\n",
    "                break\n",
    "        assert len(new_beam) >= 1, 'Non-existent beam'\n",
    "\n",
    "        return new_beam, torch.LongTensor([next_inp]), results, torch.cat(next_h, 0), torch.cat(next_c, 0)\n",
    "\n",
    "    def decode(self, encoder_outputs, decoder_hidden, decoder_cell_state, encoder_inputs):\n",
    "        _input = Variable(torch.LongTensor([[sos_idx]]).cuda(), volatile=True)\n",
    "        decoded_outputs = []\n",
    "        all_hyps = [Hypothesis([sos_idx], None, None, 0)]\n",
    "        for _step in range(dec_max_len):\n",
    "            print(_input)\n",
    "            curr_beam_size = _input.size(0)\n",
    "            enc_states = encoder_outputs.clone()\n",
    "            beam_enc_states = enc_states.expand(curr_beam_size, enc_states.size(1), enc_states.size(2)).contiguous().detach()\n",
    "            beam_article_inds  = encoder_inputs.clone().expand(curr_beam_size, x_.size(1)).detach()\n",
    "\n",
    "            vocab_probs, next_h, next_c = self.decode_step( _input, beam_enc_states, decoder_hidden, decoder_cell_state, beam_article_inds)\n",
    "\n",
    "            all_hyps, decode_inds, decoded_outputs, init_h, init_c = self.getOverallTopk(vocab_probs, next_h.view(curr_beam_size,1,-1), \\\n",
    "                                                                                    next_c.view(curr_beam_size,1,-1), all_hyps, decoded_outputs)\n",
    "            #decode_inds.masked_fill_((decode_inds > vocab_size), unk_idx)\n",
    "            \n",
    "            decode_inds = decode_inds.t()\n",
    "            _input = Variable(decode_inds.cuda(), volatile=True)\n",
    "            decoder_hidden = Variable(init_h,volatile=True)\n",
    "            decoder_cell_state = Variable(init_c,volatile=True)\n",
    "\n",
    "        non_terminal_output = [item.full_prediction for item in all_hyps]\n",
    "        all_outputs = decoded_outputs + non_terminal_output\n",
    "        return all_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_size, embedding_size, hidden_size, model_embedding).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Pointer_Decoder(vocab_size, embedding_size, hidden_size,model_embedding,lamda = 1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Encoder(\n",
       "   (embed): Embedding(50000, 128)\n",
       "   (lstm): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
       " ), Pointer_Decoder(\n",
       "   (embed): Embedding(50000, 128)\n",
       "   (lstm): LSTM(128, 512, batch_first=True)\n",
       "   (V2V): Linear(in_features=1024, out_features=1024)\n",
       "   (V): Linear(in_features=1024, out_features=50000)\n",
       "   (Wh): Linear(in_features=512, out_features=512)\n",
       "   (Ws): Linear(in_features=512, out_features=512)\n",
       "   (w_c): Linear(in_features=1, out_features=512)\n",
       "   (v): Linear(in_features=512, out_features=1)\n",
       "   (w_h): Linear(in_features=512, out_features=1)\n",
       "   (w_s): Linear(in_features=512, out_features=1)\n",
       "   (w_x): Linear(in_features=128, out_features=1)\n",
       " ))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder,decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLL = torch.nn.NLLLoss(ignore_index = pad_idx)\n",
    "\n",
    "optimizer_encoder = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "optimizer_decoder = torch.optim.Adam(decoder.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def batch(batch_size,input_var,dec_input_var,target_var,length_var, dec_length_var):\n",
    "\n",
    "    shuffle_list = list(zip(input_var,dec_input_var,target_var,length_var,dec_length_var))\n",
    "    random.shuffle(shuffle_list)\n",
    "    \n",
    "    start = 0\n",
    "    end = batch_size\n",
    "    #if len(input_var)%32 != 0:\n",
    "    while end < len(input_var):\n",
    "        batch_input = []\n",
    "        batch_input_dec = []\n",
    "        batch_target = []\n",
    "        batch_length = []\n",
    "        batch_length_dec = []\n",
    "        \n",
    "        batch_shuffle = shuffle_list[start:end]\n",
    "        \n",
    "        for i,j,k,n,m in batch_shuffle:\n",
    "            batch_input.append(i)\n",
    "            batch_input_dec.append(j)\n",
    "            batch_target.append(k)\n",
    "            batch_length.append(n)\n",
    "            batch_length_dec.append(m)\n",
    "            \n",
    "        temp = end\n",
    "        end  = end + batch_size\n",
    "        start = temp\n",
    "        yield batch_input, batch_input_dec, batch_target, batch_length, batch_length_dec\n",
    "        \n",
    "    if end >= len(input_var):\n",
    "        batch_input = []\n",
    "        batch_input_dec = []\n",
    "        batch_target = []\n",
    "        batch_length = []\n",
    "        batch_length_dec = []\n",
    "        batch_shuffle = shuffle_list[start:]\n",
    "        \n",
    "        for i,j,k,n,m in batch_shuffle:\n",
    "            batch_input.append(i)\n",
    "            batch_input_dec.append(j)\n",
    "            batch_target.append(k)\n",
    "            batch_length.append(n)\n",
    "            batch_length_dec.append(m)\n",
    "        yield batch_input, batch_input_dec, batch_target, batch_length, batch_length_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "max_target_len = 20\n",
    "clip = 2.0\n",
    "teacher_forcing_ratio = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['./chunked/train_*.bin', './chunked/val_*.bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/201 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chunked/train_*.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/ipykernel_launcher.py:49: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/ipykernel_launcher.py:53: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0010/112, Loss    5.9769, coverage Loss    0.9496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type Pointer_Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0020/112, Loss    5.8146, coverage Loss    0.9477\n",
      "Batch 0030/112, Loss    5.8074, coverage Loss    0.9470\n",
      "Batch 0040/112, Loss    5.7000, coverage Loss    0.9463\n",
      "Batch 0050/112, Loss    5.5244, coverage Loss    0.9419\n",
      "Batch 0060/112, Loss    5.3730, coverage Loss    0.9142\n",
      "Batch 0070/112, Loss    5.1640, coverage Loss    0.8416\n",
      "Batch 0080/112, Loss    5.0445, coverage Loss    0.7822\n",
      "Batch 0090/112, Loss    4.8428, coverage Loss    0.6988\n",
      "Batch 0100/112, Loss    4.6473, coverage Loss    0.5747\n",
      "Batch 0110/112, Loss    4.4479, coverage Loss    0.4380\n",
      "Batch 0112/112, Loss    4.4427, coverage Loss    0.4292\n",
      "./chunked/val_*.bin\n",
      "Valid Batch 0005/5, Loss    4.2339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 1/201 [03:46<12:36:37, 226.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at won/2018-Aug-01-01-05-16\n",
      "Epoch 00/200, Mean ELBO    4.1789\n",
      "./chunked/train_*.bin\n",
      "Batch 0010/112, Loss    4.2786, coverage Loss    0.3585\n",
      "Batch 0020/112, Loss    4.2408, coverage Loss    0.3336\n",
      "Batch 0030/112, Loss    4.0624, coverage Loss    0.3143\n",
      "Batch 0040/112, Loss    4.1015, coverage Loss    0.3072\n",
      "Batch 0050/112, Loss    4.0548, coverage Loss    0.2916\n",
      "Batch 0060/112, Loss    3.9083, coverage Loss    0.2696\n",
      "Batch 0070/112, Loss    3.9090, coverage Loss    0.2580\n",
      "Batch 0080/112, Loss    3.8889, coverage Loss    0.2447\n",
      "Batch 0090/112, Loss    3.8718, coverage Loss    0.2530\n",
      "Batch 0100/112, Loss    3.9203, coverage Loss    0.2609\n",
      "Batch 0110/112, Loss    3.8408, coverage Loss    0.2475\n",
      "Batch 0112/112, Loss    3.7961, coverage Loss    0.2337\n",
      "./chunked/val_*.bin\n",
      "Valid Batch 0005/5, Loss    3.7666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 2/201 [07:32<12:30:50, 226.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at won/2018-Aug-01-01-05-16\n",
      "Epoch 01/200, Mean ELBO    3.9182\n",
      "./chunked/train_*.bin\n",
      "Batch 0010/112, Loss    3.5523, coverage Loss    0.2243\n",
      "Batch 0020/112, Loss    3.5503, coverage Loss    0.2205\n",
      "Batch 0030/112, Loss    3.5781, coverage Loss    0.2235\n",
      "Batch 0040/112, Loss    3.4818, coverage Loss    0.2107\n",
      "Batch 0050/112, Loss    3.4982, coverage Loss    0.1977\n",
      "Batch 0060/112, Loss    3.5365, coverage Loss    0.1979\n",
      "Batch 0070/112, Loss    3.5388, coverage Loss    0.2054\n",
      "Batch 0080/112, Loss    3.5664, coverage Loss    0.1980\n",
      "Batch 0090/112, Loss    3.4976, coverage Loss    0.2107\n",
      "Batch 0100/112, Loss    3.4903, coverage Loss    0.1962\n",
      "Batch 0110/112, Loss    3.5428, coverage Loss    0.2003\n",
      "Batch 0112/112, Loss    3.4665, coverage Loss    0.1962\n",
      "./chunked/val_*.bin\n",
      "Valid Batch 0005/5, Loss    3.6257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▏         | 3/201 [11:19<12:27:23, 226.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at won/2018-Aug-01-01-05-16\n",
      "Epoch 02/200, Mean ELBO    3.7750\n",
      "./chunked/train_*.bin\n",
      "Batch 0010/112, Loss    3.2500, coverage Loss    0.1907\n",
      "Batch 0020/112, Loss    3.2510, coverage Loss    0.1853\n",
      "Batch 0030/112, Loss    3.2462, coverage Loss    0.1833\n",
      "Batch 0040/112, Loss    3.2370, coverage Loss    0.1807\n",
      "Batch 0050/112, Loss    3.1510, coverage Loss    0.1712\n",
      "Batch 0060/112, Loss    3.2314, coverage Loss    0.1751\n",
      "Batch 0070/112, Loss    3.2741, coverage Loss    0.1687\n",
      "Batch 0080/112, Loss    3.2948, coverage Loss    0.1736\n",
      "Batch 0090/112, Loss    3.1932, coverage Loss    0.1808\n",
      "Batch 0100/112, Loss    3.3089, coverage Loss    0.1760\n",
      "Batch 0110/112, Loss    3.1907, coverage Loss    0.1827\n",
      "Batch 0112/112, Loss    3.2156, coverage Loss    0.1761\n",
      "./chunked/val_*.bin\n",
      "Valid Batch 0005/5, Loss    3.5215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 4/201 [15:08<12:26:04, 227.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at won/2018-Aug-01-01-05-16\n",
      "Epoch 03/200, Mean ELBO    3.6814\n",
      "./chunked/train_*.bin\n",
      "Batch 0010/112, Loss    3.0264, coverage Loss    0.1588\n",
      "Batch 0020/112, Loss    3.0542, coverage Loss    0.1612\n",
      "Batch 0030/112, Loss    2.9294, coverage Loss    0.1584\n",
      "Batch 0040/112, Loss    2.9467, coverage Loss    0.1535\n",
      "Batch 0050/112, Loss    3.0060, coverage Loss    0.1558\n",
      "Batch 0060/112, Loss    2.9683, coverage Loss    0.1555\n",
      "Batch 0070/112, Loss    3.0201, coverage Loss    0.1571\n",
      "Batch 0080/112, Loss    2.9756, coverage Loss    0.1492\n",
      "Batch 0090/112, Loss    2.9903, coverage Loss    0.1508\n",
      "Batch 0100/112, Loss    3.0355, coverage Loss    0.1568\n",
      "Batch 0110/112, Loss    3.0442, coverage Loss    0.1566\n",
      "Batch 0112/112, Loss    2.9948, coverage Loss    0.1566\n",
      "./chunked/val_*.bin\n",
      "Valid Batch 0005/5, Loss    3.4716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 5/201 [18:57<12:23:11, 227.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at won/2018-Aug-01-01-05-16\n",
      "Epoch 04/200, Mean ELBO    3.6177\n",
      "./chunked/train_*.bin\n",
      "Batch 0010/112, Loss    2.8272, coverage Loss    0.1530\n",
      "Batch 0020/112, Loss    2.7950, coverage Loss    0.1459\n",
      "Batch 0030/112, Loss    2.7172, coverage Loss    0.1423\n",
      "Batch 0040/112, Loss    2.7827, coverage Loss    0.1358\n",
      "Batch 0050/112, Loss    2.8053, coverage Loss    0.1374\n",
      "Batch 0060/112, Loss    2.8267, coverage Loss    0.1436\n",
      "Batch 0070/112, Loss    2.8284, coverage Loss    0.1382\n",
      "Batch 0080/112, Loss    2.8500, coverage Loss    0.1418\n",
      "Batch 0090/112, Loss    2.8646, coverage Loss    0.1403\n",
      "Batch 0100/112, Loss    2.8428, coverage Loss    0.1375\n",
      "Batch 0110/112, Loss    2.7710, coverage Loss    0.1370\n",
      "Batch 0112/112, Loss    2.8138, coverage Loss    0.1348\n",
      "./chunked/val_*.bin\n",
      "Valid Batch 0005/5, Loss    3.4268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 6/201 [22:45<12:19:23, 227.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at won/2018-Aug-01-01-05-16\n",
      "Epoch 05/200, Mean ELBO    3.5712\n",
      "./chunked/train_*.bin\n",
      "Batch 0010/112, Loss    2.5669, coverage Loss    0.1289\n",
      "Batch 0020/112, Loss    2.5166, coverage Loss    0.1256\n",
      "Batch 0030/112, Loss    2.5214, coverage Loss    0.1260\n",
      "Batch 0040/112, Loss    2.5885, coverage Loss    0.1309\n",
      "Batch 0050/112, Loss    2.6357, coverage Loss    0.1279\n",
      "Batch 0060/112, Loss    2.6249, coverage Loss    0.1277\n",
      "Batch 0070/112, Loss    2.5938, coverage Loss    0.1246\n",
      "Batch 0080/112, Loss    2.6676, coverage Loss    0.1290\n",
      "Batch 0090/112, Loss    2.6971, coverage Loss    0.1306\n",
      "Batch 0100/112, Loss    2.5993, coverage Loss    0.1208\n",
      "Batch 0110/112, Loss    2.6293, coverage Loss    0.1182\n",
      "Batch 0112/112, Loss    2.6929, coverage Loss    0.1271\n",
      "./chunked/val_*.bin\n",
      "Valid Batch 0005/5, Loss    3.4167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 7/201 [26:31<12:15:17, 227.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at won/2018-Aug-01-01-05-16\n",
      "Epoch 06/200, Mean ELBO    3.5402\n",
      "./chunked/train_*.bin\n",
      "Batch 0010/112, Loss    2.4383, coverage Loss    0.1149\n",
      "Batch 0020/112, Loss    2.4080, coverage Loss    0.1119\n",
      "Batch 0030/112, Loss    2.4884, coverage Loss    0.1102\n",
      "Batch 0040/112, Loss    2.5177, coverage Loss    0.1120\n",
      "Batch 0050/112, Loss    2.4045, coverage Loss    0.1123\n",
      "Batch 0060/112, Loss    2.4662, coverage Loss    0.1124\n",
      "Batch 0070/112, Loss    2.5537, coverage Loss    0.1110\n",
      "Batch 0080/112, Loss    2.4999, coverage Loss    0.1113\n",
      "Batch 0090/112, Loss    2.4847, coverage Loss    0.1112\n",
      "Batch 0100/112, Loss    2.5186, coverage Loss    0.1133\n",
      "Batch 0110/112, Loss    2.5265, coverage Loss    0.1141\n",
      "Batch 0112/112, Loss    2.5195, coverage Loss    0.1132\n",
      "./chunked/val_*.bin\n",
      "Valid Batch 0005/5, Loss    3.5587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 8/201 [30:19<12:11:46, 227.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at won/2018-Aug-01-01-05-16\n",
      "Epoch 07/200, Mean ELBO    3.5238\n",
      "./chunked/train_*.bin\n",
      "Batch 0010/112, Loss    2.2624, coverage Loss    0.1064\n",
      "Batch 0020/112, Loss    2.3452, coverage Loss    0.1016\n",
      "Batch 0030/112, Loss    2.2612, coverage Loss    0.1019\n",
      "Batch 0040/112, Loss    2.3212, coverage Loss    0.0978\n",
      "Batch 0050/112, Loss    2.3420, coverage Loss    0.0979\n",
      "Batch 0060/112, Loss    2.2926, coverage Loss    0.0970\n",
      "Batch 0070/112, Loss    2.3038, coverage Loss    0.1005\n",
      "Batch 0080/112, Loss    2.4067, coverage Loss    0.1032\n",
      "Batch 0090/112, Loss    2.3581, coverage Loss    0.1055\n",
      "Batch 0100/112, Loss    2.3660, coverage Loss    0.1036\n",
      "Batch 0110/112, Loss    2.3558, coverage Loss    0.0996\n",
      "Batch 0112/112, Loss    2.4054, coverage Loss    0.0988\n",
      "./chunked/val_*.bin\n",
      "Valid Batch 0005/5, Loss    3.4622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 9/201 [34:08<12:08:27, 227.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at won/2018-Aug-01-01-05-16\n",
      "Epoch 08/200, Mean ELBO    3.5114\n",
      "./chunked/train_*.bin\n",
      "Batch 0010/112, Loss    2.2020, coverage Loss    0.0970\n",
      "Batch 0020/112, Loss    2.1455, coverage Loss    0.0919\n",
      "Batch 0030/112, Loss    2.1556, coverage Loss    0.0903\n",
      "Batch 0040/112, Loss    2.1738, coverage Loss    0.0891\n",
      "Batch 0050/112, Loss    2.2096, coverage Loss    0.0931\n",
      "Batch 0060/112, Loss    2.1502, coverage Loss    0.0926\n",
      "Batch 0070/112, Loss    2.2073, coverage Loss    0.0915\n",
      "Batch 0080/112, Loss    2.2238, coverage Loss    0.0909\n",
      "Batch 0090/112, Loss    2.2973, coverage Loss    0.0922\n",
      "Batch 0100/112, Loss    2.2098, coverage Loss    0.0924\n",
      "Batch 0110/112, Loss    2.2690, coverage Loss    0.0899\n",
      "Batch 0112/112, Loss    2.2787, coverage Loss    0.0894\n",
      "./chunked/val_*.bin\n",
      "Valid Batch 0005/5, Loss    3.5326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▍         | 10/201 [37:56<12:04:33, 227.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at won/2018-Aug-01-01-05-16\n",
      "Epoch 09/200, Mean ELBO    3.5093\n",
      "./chunked/train_*.bin\n",
      "Batch 0010/112, Loss    2.0423, coverage Loss    0.0863\n",
      "Batch 0020/112, Loss    2.0620, coverage Loss    0.0860\n",
      "Batch 0030/112, Loss    2.0845, coverage Loss    0.0878\n",
      "Batch 0040/112, Loss    2.1189, coverage Loss    0.0790\n",
      "Batch 0050/112, Loss    2.1080, coverage Loss    0.0828\n",
      "Batch 0060/112, Loss    2.0512, coverage Loss    0.0834\n",
      "Batch 0070/112, Loss    2.1264, coverage Loss    0.0890\n",
      "Batch 0080/112, Loss    2.1459, coverage Loss    0.0869\n",
      "Batch 0090/112, Loss    2.0990, coverage Loss    0.0835\n",
      "Batch 0100/112, Loss    2.1418, coverage Loss    0.0800\n",
      "Batch 0110/112, Loss    2.1816, coverage Loss    0.0858\n",
      "Batch 0112/112, Loss    2.1859, coverage Loss    0.0825\n",
      "./chunked/val_*.bin\n",
      "Valid Batch 0005/5, Loss    3.5926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 11/201 [41:43<12:00:46, 227.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at won/2018-Aug-01-01-05-16\n",
      "Epoch 10/200, Mean ELBO    3.5089\n",
      "./chunked/train_*.bin\n",
      "Batch 0010/112, Loss    1.9135, coverage Loss    0.0765\n",
      "Batch 0020/112, Loss    1.9369, coverage Loss    0.0749\n",
      "Batch 0030/112, Loss    1.9587, coverage Loss    0.0760\n",
      "Batch 0040/112, Loss    2.0085, coverage Loss    0.0754\n",
      "Batch 0050/112, Loss    1.9731, coverage Loss    0.0728\n",
      "Batch 0060/112, Loss    1.9713, coverage Loss    0.0743\n",
      "Batch 0070/112, Loss    1.9745, coverage Loss    0.0733\n",
      "Batch 0080/112, Loss    1.9983, coverage Loss    0.0753\n",
      "Batch 0090/112, Loss    2.0030, coverage Loss    0.0765\n",
      "Batch 0100/112, Loss    2.0371, coverage Loss    0.0787\n",
      "Batch 0110/112, Loss    2.0001, coverage Loss    0.0741\n",
      "Batch 0112/112, Loss    2.0213, coverage Loss    0.0759\n",
      "./chunked/val_*.bin\n",
      "Valid Batch 0005/5, Loss    3.7255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 12/201 [45:30<11:56:52, 227.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at won/2018-Aug-01-01-05-16\n",
      "Epoch 11/200, Mean ELBO    3.5163\n",
      "./chunked/train_*.bin\n",
      "Batch 0010/112, Loss    1.8601, coverage Loss    0.0733\n",
      "Batch 0020/112, Loss    1.7991, coverage Loss    0.0683\n",
      "Batch 0030/112, Loss    1.8892, coverage Loss    0.0690\n",
      "Batch 0040/112, Loss    1.8506, coverage Loss    0.0678\n",
      "Batch 0050/112, Loss    1.9108, coverage Loss    0.0692\n",
      "Batch 0060/112, Loss    1.9104, coverage Loss    0.0676\n",
      "Batch 0070/112, Loss    1.9083, coverage Loss    0.0660\n",
      "Batch 0080/112, Loss    1.9064, coverage Loss    0.0698\n",
      "Batch 0090/112, Loss    1.8824, coverage Loss    0.0673\n",
      "Batch 0100/112, Loss    1.9595, coverage Loss    0.0654\n",
      "Batch 0110/112, Loss    1.9375, coverage Loss    0.0660\n",
      "Batch 0112/112, Loss    1.9665, coverage Loss    0.0677\n",
      "./chunked/val_*.bin\n",
      "Valid Batch 0005/5, Loss    3.7388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▋         | 13/201 [49:18<11:53:11, 227.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at won/2018-Aug-01-01-05-16\n",
      "Epoch 12/200, Mean ELBO    3.5239\n",
      "./chunked/train_*.bin\n",
      "Batch 0010/112, Loss    1.7231, coverage Loss    0.0621\n",
      "Batch 0020/112, Loss    1.7144, coverage Loss    0.0611\n",
      "Batch 0030/112, Loss    1.7637, coverage Loss    0.0609\n",
      "Batch 0040/112, Loss    1.8077, coverage Loss    0.0636\n",
      "Batch 0050/112, Loss    1.8114, coverage Loss    0.0617\n",
      "Batch 0060/112, Loss    1.7683, coverage Loss    0.0590\n",
      "Batch 0070/112, Loss    1.8153, coverage Loss    0.0612\n",
      "Batch 0080/112, Loss    1.8322, coverage Loss    0.0612\n",
      "Batch 0090/112, Loss    1.8663, coverage Loss    0.0617\n",
      "Batch 0100/112, Loss    1.8455, coverage Loss    0.0631\n",
      "Batch 0110/112, Loss    1.8858, coverage Loss    0.0655\n",
      "Batch 0112/112, Loss    1.8850, coverage Loss    0.0615\n",
      "./chunked/val_*.bin\n",
      "Valid Batch 0005/5, Loss    3.8539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 14/201 [53:07<11:49:40, 227.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at won/2018-Aug-01-01-05-16\n",
      "Epoch 13/200, Mean ELBO    3.5374\n",
      "./chunked/train_*.bin\n",
      "Batch 0010/112, Loss    1.7029, coverage Loss    0.0590\n",
      "Batch 0020/112, Loss    1.7124, coverage Loss    0.0571\n",
      "Batch 0030/112, Loss    1.6960, coverage Loss    0.0573\n",
      "Batch 0040/112, Loss    1.7211, coverage Loss    0.0560\n",
      "Batch 0050/112, Loss    1.6589, coverage Loss    0.0542\n",
      "Batch 0060/112, Loss    1.6795, coverage Loss    0.0548\n",
      "Batch 0070/112, Loss    1.7096, coverage Loss    0.0576\n",
      "Batch 0080/112, Loss    1.7381, coverage Loss    0.0534\n",
      "Batch 0090/112, Loss    1.7635, coverage Loss    0.0572\n",
      "Batch 0100/112, Loss    1.7362, coverage Loss    0.0537\n",
      "Batch 0110/112, Loss    1.7850, coverage Loss    0.0558\n",
      "Batch 0112/112, Loss    1.7869, coverage Loss    0.0551\n",
      "./chunked/val_*.bin\n",
      "Valid Batch 0005/5, Loss    3.8050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 15/201 [56:55<11:45:53, 227.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at won/2018-Aug-01-01-05-16\n",
      "Epoch 14/200, Mean ELBO    3.5519\n",
      "./chunked/train_*.bin\n",
      "Batch 0010/112, Loss    1.5877, coverage Loss    0.0513\n",
      "Batch 0020/112, Loss    1.5674, coverage Loss    0.0512\n",
      "Batch 0030/112, Loss    1.6097, coverage Loss    0.0522\n",
      "Batch 0040/112, Loss    1.6238, coverage Loss    0.0478\n",
      "Batch 0050/112, Loss    1.6845, coverage Loss    0.0523\n",
      "Batch 0060/112, Loss    1.7038, coverage Loss    0.0540\n",
      "Batch 0070/112, Loss    1.6523, coverage Loss    0.0522\n",
      "Batch 0080/112, Loss    1.6892, coverage Loss    0.0513\n",
      "Batch 0090/112, Loss    1.6367, coverage Loss    0.0516\n",
      "Batch 0100/112, Loss    1.6660, coverage Loss    0.0542\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "var_losses = []\n",
    "train_losses = []\n",
    "avg_losses = []\n",
    "coverege_losses = []\n",
    "iteration = 0\n",
    "lamda = 1\n",
    "for epoch in tqdm.tqdm(range(epochs+1)):\n",
    "    optimizer_encoder.zero_grad()\n",
    "    optimizer_decoder.zero_grad()\n",
    "    for path in paths :\n",
    "        print(path)\n",
    "        input_index, inputs_len, dec_inputs_index, dec_inputs_len, target_index = index_data[path]\n",
    "        \n",
    "        for batch_x, batch_y_x, batch_y, batch_len, batch_len_y in batch(batch_size_fit, input_index, \\\n",
    "                                                                         dec_inputs_index,target_index, \\\n",
    "                                                                         inputs_len, dec_inputs_len):\n",
    "            optimizer_encoder.zero_grad()\n",
    "            optimizer_decoder.zero_grad()\n",
    "\n",
    "            iteration = iteration + 1\n",
    "\n",
    "            if path == paths[0]:\n",
    "                encoder.train()\n",
    "                decoder.train()\n",
    "            else:\n",
    "                encoder.eval()\n",
    "                decoder.eval()\n",
    "\n",
    "            x_ = Variable(torch.cuda.LongTensor(batch_x))\n",
    "            #x_index = Variable(torch.cuda.LongTensor(batch_x))\n",
    "            y_ = Variable(torch.cuda.LongTensor(batch_y)).transpose(1,0)\n",
    "            batch_size = x_.size(0)\n",
    "\n",
    "\n",
    "            encoder_outputs, (encoder_hidden, encoder_cell_state) = encoder(x_)\n",
    "            \n",
    "            decoder_input=Variable(torch.cuda.LongTensor(batch_size)).fill_(sos_idx)\n",
    "            decoder_hidden = encoder_hidden#[-2:]\n",
    "            decoder_cell_state = encoder_cell_state#[-2:]\n",
    "            \n",
    "            coverage =  Variable(torch.zeros(batch_size, enc_max_len)).cuda()\n",
    "            \n",
    "            loss = 0\n",
    "            coverege_loss_ = 0\n",
    "            use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "            if use_teacher_forcing:# or path == paths[1]:\n",
    "                print('use_teacher_forcing, '+path)\n",
    "                for i in range(max_target_len):\n",
    "                    P_vocab, decoder_hidden, decoder_cell_state, attn_scores, p_gen, attn_scores = decoder(\\\n",
    "                                                        decoder_input,encoder_outputs, decoder_hidden, decoder_cell_state,\\\n",
    "                                                                                                        x_,coverage)\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    _c_loss , index=torch.stack((coverage,attn_scores),2).min(2)\n",
    "                    #print(index[0])\n",
    "                    coverege_loss = _c_loss.type(torch.cuda.DoubleTensor).sum(1)\n",
    "                    coverege_loss = coverege_loss.sum().div(batch_size)\n",
    "                    coverage =coverage+ attn_scores\n",
    "                    topv, topi = P_vocab.data.topk(1)\n",
    "\n",
    "                    nllloss = NLL(torch.log(P_vocab),y_[i])\n",
    "                    \n",
    "                    total_loss = nllloss + lamda*coverege_loss.type(torch.cuda.FloatTensor)\n",
    "                    loss += total_loss\n",
    "                    \n",
    "                    coverege_loss_ += coverege_loss\n",
    "                    #decoder_hidden = decoder_hidden.squeeze(0)\n",
    "                    decoder_input = Variable(topi).squeeze(0)\n",
    "            else:\n",
    "                for i in range(max_target_len):\n",
    "                    #print(coverage)\n",
    "                    P_vocab, decoder_hidden, decoder_cell_state, attn_scores, p_gen, attn_scores = decoder(\\\n",
    "                                                                decoder_input,encoder_outputs, decoder_hidden, decoder_cell_state,\\\n",
    "                                                                                                           x_,coverage)\n",
    "                    #print('attn_scores')\n",
    "                    #print(attn_scores)\n",
    "                    \n",
    "                    _c_loss , index=torch.stack((coverage,attn_scores),2).min(2)\n",
    "                    #print(index[0].sum())\n",
    "                    coverege_loss = _c_loss.type(torch.cuda.DoubleTensor).sum(1)\n",
    "                    coverege_loss = coverege_loss.sum().div(batch_size)\n",
    "                    coverage =coverage+ attn_scores\n",
    "                    topv, topi = P_vocab.data.topk(1)\n",
    "\n",
    "                    nllloss = NLL(torch.log(P_vocab),y_[i])\n",
    "                    total_loss = nllloss + lamda*coverege_loss.type(torch.cuda.FloatTensor)\n",
    "                    loss += total_loss\n",
    "                    \n",
    "                    coverege_loss_ += coverege_loss\n",
    "                    #decoder_hidden = decoder_hidden.squeeze(0)\n",
    "                    decoder_input = y_[i]\n",
    "            \n",
    "            if path == paths[0]:\n",
    "\n",
    "                loss.backward()\n",
    "                #print('a')\n",
    "                torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "                torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "                optimizer_encoder.step()\n",
    "                optimizer_decoder.step()\n",
    "                \n",
    "                loss = loss.data[0]/max_target_len\n",
    "                c_loss = coverege_loss_.data[0]/max_target_len\n",
    "                coverege_losses.append(c_loss)\n",
    "                train_losses.append(loss)\n",
    "                \n",
    "\n",
    "                \n",
    "                step += 1\n",
    "\n",
    "                if iteration % 10 == 0 or iteration == (len(input_index)-1)//batch_size:\n",
    "                    print(\"Batch %04d/%i, Loss %9.4f, coverage Loss %9.4f\"%( iteration, (len(input_index)-1)//batch_size_fit, loss, c_loss))\n",
    "                    np.savez(L=train_losses,file='./train_loss.npz')\n",
    "                    np.savez(L=coverege_losses,file='./coverege_loss.npz')\n",
    "                    checkpoint_path_encoder = os.path.join(save_model_path, \"enc_E%i.pytorch\"%(epoch))\n",
    "                    checkpoint_path_decoder = os.path.join(save_model_path, \"dec_E%i.pytorch\"%(epoch))\n",
    "                    torch.save(encoder, checkpoint_path_encoder)\n",
    "                    torch.save(decoder, checkpoint_path_decoder)\n",
    "                    \n",
    "\n",
    "\n",
    "                del nllloss\n",
    "                del loss\n",
    "                del x_\n",
    "                del y_\n",
    "                del total_loss\n",
    "                \n",
    "            else:\n",
    "\n",
    "                loss = loss.data[0]/max_target_len\n",
    "\n",
    "                var_losses.append(loss)\n",
    "\n",
    "                step += 1\n",
    "\n",
    "                if iteration % 10 == 0 or iteration == (len(input_index)-1)//batch_size:\n",
    "                    print(\"Valid Batch %04d/%i, Loss %9.4f\"%( iteration, (len(input_index)-1)//batch_size_fit, loss))\n",
    "                    np.savez(L=var_losses,file='./var_loss.npz')\n",
    "                    \n",
    "\n",
    "\n",
    "                del nllloss\n",
    "                del loss\n",
    "                del x_\n",
    "                del y_\n",
    "                del total_loss\n",
    "                \n",
    "            optimizer_encoder.zero_grad()\n",
    "            optimizer_decoder.zero_grad()\n",
    "        iteration = 0\n",
    "        \n",
    "    print(\"Model saved at %s\"%save_model_path)\n",
    "    print(\"Epoch %02d/%i, Mean ELBO %9.4f\"%( epoch, epochs, np.mean(np.array(var_losses))))\n",
    "    avg_losses.append(np.mean(np.array(var_losses)))\n",
    "    np.savez(L=avg_losses,file='./avg_losses.npz')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path,epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = os.path.join('won', '2018-Jul-31-09-36-35')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path_encoder = os.path.join(save_model_path, 'enc_E22.pytorch')\n",
    "checkpoint_path_decoder = os.path.join(save_model_path, 'dec_E22.pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = torch.load(checkpoint_path_encoder)\n",
    "decoder = torch.load(checkpoint_path_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "print(encoder,decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# greedy generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(input_index,inputs_len,dec_inputs_index,dec_inputs_len,target_index) = index_data[files_group[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "end = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = Variable(torch.cuda.LongTensor(input_index[start:end]))\n",
    "\n",
    "batch_size = x_.size(0)\n",
    "\n",
    "encoder_outputs, (encoder_hidden, encoder_cell_state) = encoder(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input=Variable(torch.cuda.LongTensor(batch_size)).fill_(sos_idx)\n",
    "decoder_hidden = encoder_hidden#[-2:]\n",
    "decoder_cell_state = encoder_cell_state#[-2:]\n",
    "coverage =  Variable(torch.zeros(batch_size, enc_max_len)).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a=decoder.decode( encoder_outputs, decoder_hidden, decoder_cell_state, x_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b=list(map(vocab.id2word,a[0]))\n",
    "c=list(map(vocab.id2word,a[1]))\n",
    "d=list(map(vocab.id2word,target_index[start:end][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rouge.get_scores(' '.join(d), ' '.join(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "decoded_words = []\n",
    "decoder_attentions = torch.zeros(encoder_outputs.size(0),dec_max_len+20, enc_max_len)\n",
    "for i in range(dec_max_len):\n",
    "    #print(decoder_input,decoder_hidden)\n",
    "    P_vocab, decoder_hidden, decoder_cell_state, attn_scores, p_gen, decoder_attention = decoder(\\\n",
    "                                                        decoder_input,encoder_outputs, decoder_hidden, decoder_cell_state,\\\n",
    "                                                                                                        x_,coverage)\n",
    "    coverage = coverage+attn_scores\n",
    "    decoder_attentions[:attn_scores.size(0),i,:attn_scores.size(1)] += attn_scores.cpu().data\n",
    "    topv, topi = P_vocab.data.topk(1)\n",
    "    #ni = topi[0]\n",
    "    '''\n",
    "    if ni == eos_idx:\n",
    "        decoded_words.append(_EOS_)\n",
    "        break\n",
    "    else:\n",
    "        decoded_words.append(index2word[ni])\n",
    "    '''\n",
    "    temp = []\n",
    "    if len(topi.size()) == 1:\n",
    "        temp.append(vocab.id2word(topi.cpu().numpy()[0]))\n",
    "    else:\n",
    "        for top in topi.cpu().numpy():\n",
    "            temp.append(vocab.id2word(top[0]))\n",
    "    decoded_words.append(temp)\n",
    "    decoder_input = Variable(topi).cuda()\n",
    "    decoder_hidden = decoder_hidden.squeeze()\n",
    "    if len(decoder_hidden.size()) == 1:\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(0)\n",
    "    else: \n",
    "        decoder_hidden = decoder_hidden\n",
    "del coverage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferences = []\n",
    "for decoded_sent in np.array(decoded_words).transpose():\n",
    "    for i,word in enumerate(decoded_sent):\n",
    "        if word == STOP_DECODING:\n",
    "            decoded_sent = decoded_sent[:i+1]\n",
    "\n",
    "    inferences.append(list(decoded_sent))\n",
    "\n",
    "\n",
    "targets_result = []\n",
    "for inputs in target_index[start:end]:\n",
    "    result = []\n",
    "    for word in inputs:\n",
    "        if word == eos_idx:\n",
    "            break\n",
    "        else:\n",
    "            result.append(vocab.id2word(word))\n",
    "    targets_result.append(result)    \n",
    "\n",
    "inputs_result = []\n",
    "for inputs in input_index[start:end]:\n",
    "    result = []\n",
    "    for word in inputs:\n",
    "        result.append(vocab.id2word(word))\n",
    "    inputs_result.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source,target,inference, decoder_attention in zip(inputs_result[:10], targets_result[:10], inferences[:10], decoder_attentions[:10]):\n",
    "    print('*********************************************************************')\n",
    "    print('source : '+' '.join(source))\n",
    "    \n",
    "    print('*********************************************************************')\n",
    "    print('target : '+' '.join(target))\n",
    "    \n",
    "    print('*********************************************************************')\n",
    "    print('inference : '+' '.join(inference))\n",
    "    \n",
    "    print('*********************************************************************')\n",
    "    scores = rouge.get_scores(' '.join(target), ' '.join(inference))\n",
    "    for score in scores[0]:\n",
    "        print(score +' : '+str(scores[0][score]))\n",
    "\n",
    "    plt.matshow(decoder_attention.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r1_recall = []\n",
    "r2_recall = []\n",
    "rl_recall = []\n",
    "\n",
    "r1_f1 = []\n",
    "r2_f1 = []\n",
    "rl_f1 = []\n",
    "\n",
    "error = 0\n",
    "\n",
    "for source,target,inference in tqdm.tqdm(zip(inputs_result, targets_result, inferences)):#, decoder_attentions)):\n",
    "    try:\n",
    "        scores = rouge.get_scores(' '.join(target), ' '.join(inference))\n",
    "    except ValueError:\n",
    "        error +=1\n",
    "        print('******************************************')\n",
    "        print('target'+' '.join(target))\n",
    "        print('inference'+' '.join(inference))\n",
    "        pass\n",
    "    r1_recall.append(scores[0]['rouge-1']['r'])\n",
    "    r2_recall.append(scores[0]['rouge-2']['r'])\n",
    "    rl_recall.append(scores[0]['rouge-l']['r'])\n",
    "    \n",
    "    r1_f1.append(scores[0]['rouge-1']['f'])\n",
    "    r2_f1.append(scores[0]['rouge-2']['f'])\n",
    "    rl_f1.append(scores[0]['rouge-l']['f'])\n",
    "print('error sentence : ' + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ROUGE-1 recall : ' + str(sum(r1_recall)*100 / len(r1_recall)))\n",
    "print('ROUGE-1 F1 : ' + str(sum(r1_f1)*100 / len(r1_f1)))\n",
    "print('********************************************************************')\n",
    "print('ROUGE-2 recall : ' + str(sum(r2_recall)*100 / len(r2_recall)))\n",
    "print('ROUGE-2 F1 : ' + str(sum(r2_f1)*100 / len(r2_f1)))\n",
    "print('********************************************************************')\n",
    "print('ROUGE-l recall : ' + str(sum(rl_recall)*100 / len(rl_recall)))\n",
    "print('ROUGE-l F1 : ' + str(sum(rl_f1)*100 / len(rl_f1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 'won/2018-Jul-30-11-30-40' epoch 9\n",
    "ROUGE-1 recall : 25.147463257859798\n",
    "ROUGE-1 F1 : 26.152656594511935\n",
    "********************************************************************\n",
    "ROUGE-2 recall : 11.111168986470044\n",
    "ROUGE-2 F1 : 11.87687849599647\n",
    "********************************************************************\n",
    "ROUGE-l recall : 23.374466815236698\n",
    "ROUGE-l F1 : 23.332530990714563"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 'won/2018-Jul-31-01-09-08' epoch18\n",
    "ROUGE-1 recall : 27.181338680580858\n",
    "ROUGE-1 F1 : 23.24354440201661\n",
    "********************************************************************\n",
    "ROUGE-2 recall : 10.285292567935505\n",
    "ROUGE-2 F1 : 10.084292863650939\n",
    "********************************************************************\n",
    "ROUGE-l recall : 25.50797278681479\n",
    "ROUGE-l F1 : 20.400153250444962"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 'won', '2018-Jul-31-06-07-54' epoch 14\n",
    "ROUGE-1 recall : 24.045028589331288\n",
    "ROUGE-1 F1 : 26.872889411505124\n",
    "********************************************************************\n",
    "ROUGE-2 recall : 10.375095347295948\n",
    "ROUGE-2 F1 : 11.756716997867436\n",
    "********************************************************************\n",
    "ROUGE-l recall : 22.083905535277424\n",
    "ROUGE-l F1 : 23.46146722706788"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
