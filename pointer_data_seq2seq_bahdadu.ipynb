{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gensim\n",
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "%matplotlib  inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from rouge import Rouge \n",
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% time\n",
    "with open('./data_pointer_example.txt', 'r', encoding='UTF-8') as f:\n",
    "    stories = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embedding = np.load('Word2vec_pointer.npz')['E']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.strftime('%Y-%b-%d-%H-%M-%S', time.gmtime())\n",
    "\n",
    "save_model_path = os.path.join('won', ts)\n",
    "os.makedirs('./'+save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "\n",
    "  def __init__(self, vocab_file, max_size):\n",
    "    self._word_to_id = {}\n",
    "    self._id_to_word = {}\n",
    "    self._count = 0 # keeps track of total number of words in the Vocab\n",
    "\n",
    "    # [UNK], [PAD], [START] and [STOP] get the ids 0,1,2,3.\n",
    "    for w in [UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
    "      self._word_to_id[w] = self._count\n",
    "      self._id_to_word[self._count] = w\n",
    "      self._count += 1\n",
    "\n",
    "    # Read the vocab file and add words up to max_size\n",
    "    with open(vocab_file, 'r', encoding='utf-8') as vocab_f:\n",
    "      for line in vocab_f:\n",
    "        pieces = line.split()\n",
    "        if len(pieces) != 2:\n",
    "          print ('Warning: incorrectly formatted line in vocabulary file: %s\\n' % line)\n",
    "          continue\n",
    "        w = pieces[0]\n",
    "        if w in [SENTENCE_START, SENTENCE_END, UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
    "          raise Exception('<s>, </s>, [UNK], [PAD], [START] and [STOP] shouldn\\'t be in the vocab file, but %s is' % w)\n",
    "        if w in self._word_to_id:\n",
    "          raise Exception('Duplicated word in vocabulary file: %s' % w)\n",
    "        self._word_to_id[w] = self._count\n",
    "        self._id_to_word[self._count] = w\n",
    "        self._count += 1\n",
    "        if max_size != 0 and self._count >= max_size:\n",
    "          print (\"max_size of vocab was specified as %i; we now have %i words. Stopping reading.\" % (max_size, self._count))\n",
    "          break\n",
    "\n",
    "    print (\"Finished constructing vocabulary of %i total words. Last word added: %s\" % (self._count, self._id_to_word[self._count-1]))\n",
    "\n",
    "  def word2id(self, word):\n",
    "    if word not in self._word_to_id:\n",
    "      return self._word_to_id[UNKNOWN_TOKEN]\n",
    "    return self._word_to_id[word]\n",
    "\n",
    "  def id2word(self, word_id):\n",
    "    if word_id not in self._id_to_word:\n",
    "      raise ValueError('Id not found in vocab: %d' % word_id)\n",
    "    return self._id_to_word[word_id]\n",
    "\n",
    "  def size(self):\n",
    "    return self._count\n",
    "\n",
    "  def write_metadata(self, fpath):\n",
    "    print (\"Writing word embedding metadata file to %s...\" % (fpath))\n",
    "    with open(fpath, \"w\") as f:\n",
    "      fieldnames = ['word']\n",
    "      writer = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=fieldnames)\n",
    "      for i in xrange(self.size()):\n",
    "        writer.writerow({\"word\": self._id_to_word[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <s> and </s> are used in the data files to segment the abstracts into sentences. They don't receive vocab ids.\n",
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "PAD_TOKEN = '[PAD]' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNKNOWN_TOKEN = '[UNK]' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "START_DECODING = '[START]' # This has a vocab id, which is used at the start of every decoder input sequence\n",
    "STOP_DECODING = '[STOP]' # This has a vocab id, which is used at the end of untruncated target sequences\n",
    "\n",
    "# Note: none of <s>, </s>, [PAD], [UNK], [START], [STOP] should appear in the vocab file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab('./vocab', 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Example(object):\n",
    "\n",
    "  def __init__(self, article, abstract_sentences, vocab):\n",
    "    # Get ids of special tokens\n",
    "    start_decoding = vocab.word2id(START_DECODING)\n",
    "    stop_decoding = vocab.word2id(STOP_DECODING)\n",
    "\n",
    "    # Process the article\n",
    "    article_words = article.split()\n",
    "    if len(article_words) > 200:\n",
    "      article_words = article_words[:200]\n",
    "    self.enc_len = len(article_words) # store the length after truncation but before padding\n",
    "    self.enc_input = [vocab.word2id(w) for w in article_words] # list of word ids; OOVs are represented by the id for UNK token\n",
    "\n",
    "    # Process the abstract\n",
    "    abstract = ' '.join(abstract_sentences) # string\n",
    "    abstract_words = abstract.split() # list of strings\n",
    "    abs_ids = [vocab.word2id(w) for w in abstract_words] # list of word ids; OOVs are represented by the id for UNK token\n",
    "\n",
    "    # Get the decoder input sequence and target sequence\n",
    "    self.dec_input, self.target = self.get_dec_inp_targ_seqs(abs_ids, 20, start_decoding, stop_decoding)\n",
    "    self.dec_len = len(self.dec_input)\n",
    "\n",
    "    # If using pointer-generator mode, we need to store some extra info\n",
    "    if True:\n",
    "      # Store a version of the enc_input where in-article OOVs are represented by their temporary OOV id; also store the in-article OOVs words themselves\n",
    "      self.enc_input_extend_vocab, self.article_oovs = article2ids(article_words, vocab)\n",
    "\n",
    "      # Get a verison of the reference summary where in-article OOVs are represented by their temporary article OOV id\n",
    "      abs_ids_extend_vocab = abstract2ids(abstract_words, vocab, self.article_oovs)\n",
    "\n",
    "      # Overwrite decoder target sequence so it uses the temp article OOV ids\n",
    "      _, self.target = self.get_dec_inp_targ_seqs(abs_ids_extend_vocab, 20, start_decoding, stop_decoding)\n",
    "\n",
    "    # Store the original strings\n",
    "    self.original_article = article\n",
    "    self.original_abstract = abstract\n",
    "    self.original_abstract_sents = abstract_sentences\n",
    "\n",
    "\n",
    "  def get_dec_inp_targ_seqs(self, sequence, max_len, start_id, stop_id):\n",
    "    inp = [start_id] + sequence[:]\n",
    "    target = sequence[:]\n",
    "    if len(inp) > max_len: # truncate\n",
    "      inp = inp[:max_len]\n",
    "      target = target[:max_len] # no end_token\n",
    "    else: # no truncation\n",
    "      target.append(stop_id) # end token\n",
    "    assert len(inp) == len(target)\n",
    "    return inp, target\n",
    "\n",
    "\n",
    "  def pad_decoder_inp_targ(self, max_len, pad_id):\n",
    "    while len(self.dec_input) < max_len:\n",
    "      self.dec_input.append(pad_id)\n",
    "    while len(self.target) < max_len:\n",
    "      self.target.append(pad_id)\n",
    "\n",
    "\n",
    "  def pad_encoder_input(self, max_len, pad_id):\n",
    "    while len(self.enc_input) < max_len:\n",
    "      self.enc_input.append(pad_id)\n",
    "    if True:\n",
    "      while len(self.enc_input_extend_vocab) < max_len:\n",
    "        self.enc_input_extend_vocab.append(pad_id)\n",
    "        \n",
    "def article2ids(article_words, vocab):\n",
    "  ids = []\n",
    "  oovs = []\n",
    "  unk_id = vocab.word2id(UNKNOWN_TOKEN)\n",
    "  for w in article_words:\n",
    "    i = vocab.word2id(w)\n",
    "    if i == unk_id: # If w is OOV\n",
    "      if w not in oovs: # Add to list of OOVs\n",
    "        oovs.append(w)\n",
    "      oov_num = oovs.index(w) # This is 0 for the first article OOV, 1 for the second article OOV...\n",
    "      ids.append(vocab.size() + oov_num) # This is e.g. 50000 for the first article OOV, 50001 for the second...\n",
    "    else:\n",
    "      ids.append(i)\n",
    "  return ids, oovs\n",
    "\n",
    "\n",
    "def abstract2ids(abstract_words, vocab, article_oovs):\n",
    "  ids = []\n",
    "  unk_id = vocab.word2id(UNKNOWN_TOKEN)\n",
    "  for w in abstract_words:\n",
    "    i = vocab.word2id(w)\n",
    "    if i == unk_id: # If w is an OOV word\n",
    "        #print(abstract_words)\n",
    "        ids.append(i) \n",
    "#       if w in article_oovs: # If w is an in-article OOV\n",
    "#         vocab_idx = vocab.size() + article_oovs.index(w) # Map to its temporary article OOV number\n",
    "#         ids.append(vocab_idx)\n",
    "#       else: # If w is an out-of-article OOV\n",
    "#         ids.append(unk_id) # Map to the UNK token id\n",
    "    else:\n",
    "        ids.append(i)\n",
    "  return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_decoding = vocab.word2id(START_DECODING)\n",
    "stop_decoding = vocab.word2id(STOP_DECODING)\n",
    "\n",
    "input_index=[]\n",
    "dec_inputs_index=[]\n",
    "target_index=[]\n",
    "inputs_len=[]\n",
    "dec_inputs_len=[]\n",
    "\n",
    "index_data = {}\n",
    "\n",
    "zero_len = 0\n",
    "\n",
    "dec_max_len = 20\n",
    "enc_max_len = 200\n",
    "\n",
    "pad_id = vocab.word2id(PAD_TOKEN)\n",
    "files_group = list(stories.keys())\n",
    "\n",
    "for file_group in tqdm.tqdm(files_group):\n",
    "    for file in tqdm.tqdm(list(stories[file_group].keys())):       \n",
    "        for data in list(stories[file_group][file].keys()):\n",
    "            if stories[file_group][file][data]['abstract_sentences'] == []:\n",
    "                print('*********************************************')\n",
    "                abstract = stories[file_group][file][data]['abstract']\n",
    "                abstract_sentences = []\n",
    "                article = stories[file_group][file][data]['article']\n",
    "                example=Example(article=article,abstract_sentences=abstract_sentences,vocab=vocab)\n",
    "                example.pad_decoder_inp_targ(dec_max_len,pad_id)\n",
    "                example.pad_encoder_input(enc_max_len,pad_id)\n",
    "            else:\n",
    "                abstract = stories[file_group][file][data]['abstract']\n",
    "                abstract_sentences = [stories[file_group][file][data]['abstract_sentences'][0]]\n",
    "                article = stories[file_group][file][data]['article']\n",
    "                example=Example(article=article,abstract_sentences=abstract_sentences,vocab=vocab)\n",
    "                example.pad_decoder_inp_targ(dec_max_len,pad_id)\n",
    "                example.pad_encoder_input(enc_max_len,pad_id)\n",
    "            if example.enc_len <= 0 or example.dec_len <= 0:\n",
    "                print(file_group, file, data)\n",
    "                zero_len +=1\n",
    "                pass\n",
    "            else:\n",
    "                input_index.append(example.enc_input)\n",
    "                dec_inputs_index.append(example.dec_input)\n",
    "                target_index.append(example.target)\n",
    "                inputs_len.append(example.enc_len)\n",
    "                dec_inputs_len.append(example.dec_len)\n",
    "    print('********************************************************************************')\n",
    "    print(len(input_index))\n",
    "    print('********************************************************************************')\n",
    "    index_data[file_group] = (input_index,inputs_len,dec_inputs_index,dec_inputs_len,target_index)\n",
    "    input_index=[]\n",
    "    dec_inputs_index=[]\n",
    "    target_index=[]\n",
    "    inputs_len=[]\n",
    "    dec_inputs_len=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(input_index,inputs_len,dec_inputs_index,dec_inputs_len,target_index) = index_data[files_group[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_group in files_group:\n",
    "    print(file_group)\n",
    "    (input_index,inputs_len,dec_inputs_index,dec_inputs_len,target_index) = index_data[file_group]\n",
    "    print(len(input_index))\n",
    "    start = int(len(input_index)*0.9)\n",
    "    end = -1\n",
    "    index_data[file_group] = (input_index[start:end],inputs_len[start:end],dec_inputs_index[start:end],dec_inputs_len[start:end],target_index[start:end])\n",
    "    (input_index,inputs_len,dec_inputs_index,dec_inputs_len,target_index) = index_data[file_group]\n",
    "    print(len(input_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 300\n",
    "hidden_size = 150\n",
    "word_dropout = 0.5\n",
    "num_layers = 1\n",
    "bidirectional = True\n",
    "batch_size_fit = 1024-256-128\n",
    "rnn_type = 'gru'\n",
    "learning_rate = 0.001\n",
    "\n",
    "vocab_size = len(vocab._word_to_id)\n",
    "\n",
    "# <s> and </s> are used in the data files to segment the abstracts into sentences. They don't receive vocab ids.\n",
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "PAD_TOKEN = '[PAD]' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNKNOWN_TOKEN = '[UNK]' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "START_DECODING = '[START]' # This has a vocab id, which is used at the start of every decoder input sequence\n",
    "STOP_DECODING = '[STOP]' # This has a vocab id, which is used at the end of untruncated target sequences\n",
    "\n",
    "# Note: none of <s>, </s>, [PAD], [UNK], [START], [STOP] should appear in the vocab file.\n",
    "\n",
    "sos_idx = vocab.word2id(START_DECODING)\n",
    "eos_idx = vocab.word2id(STOP_DECODING)\n",
    "pad_idx = vocab.word2id(PAD_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding_layer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, numpy_embedding = None):\n",
    "        super().__init__()\n",
    "#         if numpy_embedding == None:\n",
    "#             self.embedding = nn.Embedding(vocab_size,embedding_size)\n",
    "#         else:\n",
    "        self.model_embedding = torch.from_numpy(numpy_embedding).float()\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_size)\n",
    "        self.embedding.weight = nn.Parameter(self.model_embedding)\n",
    "            \n",
    "    def forward(self,inputs):\n",
    "        return self.embedding(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,vocab_size, embedding_size, hidden_size, word_dropout,\n",
    "                 sos_idx, eos_idx, pad_idx, embedding,rnn_type='rnn' , num_layers=1, bidirectional=True):\n",
    "        \n",
    "        super().__init__()\n",
    "        #self.model_embedding = torch.from_numpy(numpy_embedding)\n",
    "        self.sos_idx = sos_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        self.rnn_type = rnn_type\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = embedding\n",
    "        #self.embedding.weight = nn.Parameter(self.model_embedding)\n",
    "        \n",
    "        self.encoder = Encoder(vocab_size = vocab_size,embedding_size = embedding_size, hidden_size = hidden_size, num_layers = num_layers, bidirectional = bidirectional,rnn_type = rnn_type).cuda()\n",
    "        #self.decoder = Decoder(vocab_size = vocab_size,embedding_size = embedding_size, hidden_size = hidden_size, num_layers = num_layers, bidirectional = False,rnn_type = rnn_type, word_dropout=word_dropout).cuda()\n",
    "        \n",
    "    def forward(self,x,length,y_): \n",
    "        #print(x.size())\n",
    "        batch_size = x.size(0)\n",
    "        sorted_lengths, sorted_idx = torch.sort(length, descending=True)\n",
    "        input_sequence = x[sorted_idx.cuda()]\n",
    "        input_embedding = self.embedding(input_sequence).float()\n",
    "\n",
    "        packed_input = rnn_utils.pack_padded_sequence(input_embedding, sorted_lengths.tolist(), batch_first=True)\n",
    "\n",
    "        self.encoder_outputs, self.encoder_hidden_state, self.encoder_hidden_state_attn = self.encoder(packed_input,batch_size)\n",
    "\n",
    "        padded_outputs = rnn_utils.pad_packed_sequence(self.encoder_outputs, batch_first=True)[0]\n",
    "        padded_outputs = padded_outputs.contiguous()\n",
    "        _,reversed_idx = torch.sort(sorted_idx.cuda())\n",
    "        self.encoder_outputs = padded_outputs[reversed_idx]        \n",
    "        \n",
    "        return self.encoder_outputs, self.encoder_hidden_state, self.encoder_hidden_state_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_size, hidden_size, bidirectional=True, num_layers = 1,rnn_type='rnn'):\n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.rnn_type = rnn_type\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.rnn_type == 'rnn':\n",
    "            rnn = nn.RNN\n",
    "        elif self.rnn_type == 'gru':\n",
    "            rnn = nn.GRU\n",
    "        elif self.rnn_type =='lstm':\n",
    "            rnn = nn.LSTM\n",
    "        else:\n",
    "            raise ValueError()\n",
    "            \n",
    "        \n",
    "        self.encoder = rnn(self.embedding_size, self.hidden_size, num_layers = self.num_layers, bidirectional = self.bidirectional, batch_first = True)\n",
    "        self.encoder.weight_hh_l0 = nn.init.xavier_uniform(self.encoder.weight_hh_l0)\n",
    "        self.encoder.weight_ih_l0 = nn.init.xavier_uniform(self.encoder.weight_ih_l0)\n",
    "\n",
    "        self.hidden_factor = (2 if self.bidirectional else 1) * self.num_layers\n",
    "\n",
    "    \n",
    "    def forward(self,x,batch_size):\n",
    "        \n",
    "        outputs, self.hidden = self.encoder(x)\n",
    "        \n",
    "        if self.bidirectional or self.num_layers > 1:\n",
    "            # flatten hidden state\n",
    "            self.hidden_ = self.hidden.view(batch_size, self.hidden_size*self.hidden_factor)\n",
    "        else:\n",
    "            self.hidden_ = self.hidden.squeeze()\n",
    "\n",
    "        \n",
    "        return outputs, self.hidden_, self.hidden[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        stdv = 1. / math.sqrt(self.v.size(0))\n",
    "        self.v.data.normal_(mean=0, std=stdv)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        '''\n",
    "        :param hidden: \n",
    "            previous hidden state of the decoder, in shape (layers*directions,B,H)\n",
    "        :param encoder_outputs:\n",
    "            encoder outputs from Encoder, in shape (T,B,H)\n",
    "        :return\n",
    "            attention energies in shape (B,T)\n",
    "        '''\n",
    "        max_len = encoder_outputs.size(1)\n",
    "        this_batch_size = encoder_outputs.size(0)\n",
    "        #print(encoder_outputs.size())\n",
    "        #print(hidden.size())\n",
    "        self.H = hidden.repeat(max_len,1,1).transpose(0,1)\n",
    "\n",
    "        attn_energies = self.score(self.H,encoder_outputs) # compute attention score\n",
    "        return F.softmax(attn_energies).unsqueeze(1) # normalize with softmax\n",
    "\n",
    "    def score(self, hidden, encoder_outputs):\n",
    "        #print(hidden.size(), encoder_outputs.size())\n",
    "        energy = F.tanh(self.attn(torch.cat([hidden, encoder_outputs], 2))) # [B*T*2H]->[B*T*H]\n",
    "        energy = energy.transpose(2,1) # [B*H*T]\n",
    "        v = self.v.repeat(encoder_outputs.data.shape[0],1).unsqueeze(1) #[B*1*H]\n",
    "        energy = torch.bmm(v,energy) # [B*1*T]\n",
    "        return energy.squeeze(1) #[B*T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_size , hidden_size, encoder_bi, embedding, bidirectional=True, num_layers = 1,rnn_type='rnn',word_dropout = 0.5):\n",
    "        super(Decoder,self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.rnn_type = rnn_type\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = Attn('concat', hidden_size)\n",
    "        self.encoder_bi = encoder_bi\n",
    "        if self.rnn_type == 'rnn':\n",
    "            rnn = nn.RNN\n",
    "        elif self.rnn_type == 'gru':\n",
    "            rnn = nn.GRU\n",
    "        elif self.rnn_type =='lstm':\n",
    "            rnn = nn.LSTM\n",
    "        else:\n",
    "            raise ValueError()\n",
    "            \n",
    "        self.hidden_factor = (2 if self.encoder_bi else 1) * self.num_layers            \n",
    "        \n",
    "        self.embedding = embedding\n",
    "        self.word_dropout = nn.Dropout(p=word_dropout)\n",
    "        \n",
    "        self.decoder = rnn(embedding_size+hidden_size, hidden_size, num_layers=num_layers, bidirectional=self.bidirectional, batch_first=True)\n",
    "        self.decoder.weight_hh_l0 = nn.init.xavier_uniform(self.decoder.weight_hh_l0)\n",
    "        self.decoder.weight_ih_l0 = nn.init.xavier_uniform(self.decoder.weight_ih_l0)\n",
    "        \n",
    "        self.hidden2hidden = nn.Linear(hidden_size * (2 if bidirectional else 1), hidden_size//2)\n",
    "        self.hidden2outputs = nn.Linear(hidden_size//2, hidden_size//2)\n",
    "        self.outputs2vocab = nn.Linear(hidden_size//2, vocab_size)\n",
    "\n",
    "        \n",
    "    def forward(self,batch_size,hidden,encoder_outputs):\n",
    "        self.batch_size = batch_size.size(0)\n",
    "        #print(hidden.size())\n",
    "        if  self.num_layers > 1:\n",
    "            # unflatten hidden state\n",
    "            self.hidden = hidden.view(self.hidden_factor, self.batch_size, self.hidden_size)\n",
    "        else:\n",
    "            #hidden = hidden[-1]\n",
    "            self.hidden = hidden.unsqueeze(0)\n",
    "        #print(hidden.size())    \n",
    "        self.input_embedding = self.embedding(batch_size).view(encoder_outputs.size(0),1, -1)\n",
    "\n",
    "        self.input_embedding = self.word_dropout(self.input_embedding)\n",
    "        \n",
    "        self.attn_weights = self.attn(self.hidden, encoder_outputs)\n",
    "        self.context = self.attn_weights.bmm(encoder_outputs)\n",
    "\n",
    "        self.inputs = torch.cat((self.input_embedding, self.context), 2)\n",
    "        \n",
    "        self.dec_outputs,self.hidden = self.decoder(self.inputs, self.hidden)\n",
    "        #self.test = self.dec_outputs + self.context\n",
    "        self.outputs = nn.functional.log_softmax(self.outputs2vocab(self.hidden2outputs(self.hidden2hidden(self.dec_outputs.squeeze()))))\n",
    "        #self.outputs = nn.functional.log_softmax(self.outputs2vocab(self.outputs.squeeze()))\n",
    "        \n",
    "        return self.outputs, self.context, self.hidden, self.attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Embedding_layer(vocab_size, embedding_size,numpy_embedding = model_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2S=Seq2Seq(vocab_size, embedding_size, hidden_size, word_dropout,sos_idx, eos_idx, pad_idx , num_layers=num_layers ,rnn_type='gru',bidirectional= bidirectional,embedding =embedding ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_size = vocab_size,embedding_size = embedding_size, hidden_size = hidden_size*2, num_layers = num_layers, encoder_bi= bidirectional,bidirectional = False,rnn_type = rnn_type, word_dropout=word_dropout,embedding =embedding).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2S,decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLL = torch.nn.NLLLoss(ignore_index = pad_idx)\n",
    "\n",
    "optimizer_encoder = torch.optim.Adam(S2S.parameters(), lr=learning_rate)\n",
    "optimizer_decoder = torch.optim.Adam(decoder.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def batch(batch_size,input_var,dec_input_var,target_var,length_var, dec_length_var):\n",
    "\n",
    "    shuffle_list = list(zip(input_var,dec_input_var,target_var,length_var,dec_length_var))\n",
    "    random.shuffle(shuffle_list)\n",
    "    \n",
    "    start = 0\n",
    "    end = batch_size\n",
    "    #if len(input_var)%32 != 0:\n",
    "    while end < len(input_var):\n",
    "        batch_input = []\n",
    "        batch_input_dec = []\n",
    "        batch_target = []\n",
    "        batch_length = []\n",
    "        batch_length_dec = []\n",
    "        \n",
    "        batch_shuffle = shuffle_list[start:end]\n",
    "        \n",
    "        for i,j,k,n,m in batch_shuffle:\n",
    "            batch_input.append(i)\n",
    "            batch_input_dec.append(j)\n",
    "            batch_target.append(k)\n",
    "            batch_length.append(n)\n",
    "            batch_length_dec.append(m)\n",
    "            \n",
    "        temp = end\n",
    "        end  = end + batch_size\n",
    "        start = temp\n",
    "        yield batch_input, batch_input_dec, batch_target, batch_length, batch_length_dec\n",
    "        \n",
    "    if end >= len(input_var):\n",
    "        batch_input = []\n",
    "        batch_input_dec = []\n",
    "        batch_target = []\n",
    "        batch_length = []\n",
    "        batch_length_dec = []\n",
    "        batch_shuffle = shuffle_list[start:]\n",
    "        \n",
    "        for i,j,k,n,m in batch_shuffle:\n",
    "            batch_input.append(i)\n",
    "            batch_input_dec.append(j)\n",
    "            batch_target.append(k)\n",
    "            batch_length.append(n)\n",
    "            batch_length_dec.append(m)\n",
    "        yield batch_input, batch_input_dec, batch_target, batch_length, batch_length_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "max_target_len = 20\n",
    "clip = 2.0\n",
    "teacher_forcing_ratio = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "max_target_len = 20\n",
    "clip = 2.0\n",
    "teacher_forcing_ratio = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['./chunked/train_*.bin', './chunked/val_*.bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in paths :\n",
    "    print(path == './chunked/train_*.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step = 0\n",
    "var_losses = []\n",
    "train_losses = []\n",
    "avg_losses = []\n",
    "topis_list = []\n",
    "iteration = 0\n",
    "for epoch in tqdm.tqdm(range(epochs+1)):\n",
    "    \n",
    "    for path in paths :\n",
    "        print(path)\n",
    "        input_index, inputs_len, dec_inputs_index, dec_inputs_len, target_index = index_data[path]\n",
    "        \n",
    "        for batch_x, batch_y_x, batch_y, batch_len, batch_len_y in batch(batch_size_fit, input_index, \\\n",
    "                                                                         dec_inputs_index,target_index, \\\n",
    "                                                                         inputs_len, dec_inputs_len):\n",
    "            optimizer_encoder.zero_grad()\n",
    "            optimizer_decoder.zero_grad()\n",
    "\n",
    "            iteration = iteration + 1\n",
    "\n",
    "            if path == paths[0]:\n",
    "                S2S.train()\n",
    "                decoder.train()\n",
    "            else:\n",
    "                S2S.eval()\n",
    "                decoder.eval()\n",
    "\n",
    "            x_ = Variable(torch.cuda.LongTensor(batch_x))\n",
    "            y_ = Variable(torch.cuda.LongTensor(batch_y)).transpose(1,0)\n",
    "            batch_size = x_.size(0)\n",
    "\n",
    "            length = torch.cuda.LongTensor(batch_len)\n",
    "\n",
    "            decoder_input=Variable(torch.cuda.LongTensor(batch_size)).fill_(sos_idx)\n",
    "\n",
    "            encoder_outputs, encoder_hidden_state,encoder_hidden_state_attn=S2S(x_,length,y_)\n",
    "            #print(encoder_outputs)\n",
    "            decoder_hidden = encoder_hidden_state\n",
    "\n",
    "            loss = 0\n",
    "            tamp=[]\n",
    "\n",
    "            for i in range(max_target_len):\n",
    "                decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input,decoder_hidden,encoder_outputs)\n",
    "                #print(S2S.decoder.input_embedding)\n",
    "                #print(index2word[a.data.topk(1)[1].cpu().numpy()[0][0]])\n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                tamp.append(topi.cpu().numpy())\n",
    "                loss += NLL(decoder_output,y_[i])\n",
    "                decoder_hidden = decoder_hidden.squeeze(0)\n",
    "                decoder_input = y_[i]\n",
    "            topis_list.append(tamp)\n",
    "            \n",
    "            if path == paths[0]:\n",
    "\n",
    "                loss.backward()\n",
    "                #torch.nn.utils.clip_grad_norm(S2S.parameters(), clip)\n",
    "                #torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "                optimizer_encoder.step()\n",
    "                optimizer_decoder.step()\n",
    "\n",
    "                loss = loss.data[0]/max_target_len\n",
    "\n",
    "                train_losses.append(loss)\n",
    "\n",
    "                step += 1\n",
    "\n",
    "                if iteration % 10 == 0 or iteration == (len(input_index)-1)//batch_size:\n",
    "                    print(\"Batch %04d/%i, Loss %9.4f\"%( iteration, (len(input_index)-1)//batch_size_fit, loss))\n",
    "                    np.savez(L=train_losses,file='./train_loss.npz')\n",
    "                    checkpoint_path_encoder = os.path.join(save_model_path, \"enc_E%i.pytorch\"%(epoch))\n",
    "                    checkpoint_path_decoder = os.path.join(save_model_path, \"dec_E%i.pytorch\"%(epoch))\n",
    "                    torch.save(S2S, checkpoint_path_encoder)\n",
    "                    torch.save(decoder, checkpoint_path_decoder)\n",
    "\n",
    "            else:\n",
    "\n",
    "                loss = loss.data[0]/max_target_len\n",
    "\n",
    "                var_losses.append(loss)\n",
    "\n",
    "                step += 1\n",
    "\n",
    "                if iteration % 10 == 0 or iteration == (len(input_index)-1)//batch_size:\n",
    "                    print(\"Valid Batch %04d/%i, Loss %9.4f\"%( iteration, (len(input_index)-1)//batch_size_fit, loss))\n",
    "                    np.savez(L=var_losses,file='./var_loss.npz')\n",
    "\n",
    "            del loss\n",
    "            del encoder_outputs\n",
    "            del encoder_hidden_state \n",
    "        iteration = 0\n",
    "    print(\"Model saved at %s\"%save_model_path)\n",
    "    print(\"Epoch %02d/%i, Mean ELBO %9.4f\"%( epoch, epochs, np.mean(np.array(var_losses))))\n",
    "    avg_losses.append(np.mean(np.array(var_losses)))\n",
    "    np.savez(L=avg_losses,file='./avg_losses.npz')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path,epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = os.path.join('won', '2018-Jul-20-01-38-51')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path_encoder = os.path.join(save_model_path, 'enc_E9.pytorch')\n",
    "checkpoint_path_decoder = os.path.join(save_model_path, 'dec_E9.pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2S = torch.load(checkpoint_path_encoder)\n",
    "decoder = torch.load(checkpoint_path_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2S.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(input_index,inputs_len,dec_inputs_index,dec_inputs_len,target_index) = index_data[files_group[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "end = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_len = []\n",
    "for sentence in input_index[start:end]:\n",
    "    inputs_len.append(len(sentence) - sentence.count(0))\n",
    "dec_inputs_len = []\n",
    "for sentence in dec_inputs_index[start:end]:\n",
    "    dec_inputs_len.append(len(sentence) - sentence.count(vocab.word2id(PAD_TOKEN)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = Variable(torch.cuda.LongTensor(input_index[start:end]))\n",
    "Y_X = Variable(torch.cuda.LongTensor(dec_inputs_index[start:end]))\n",
    "\n",
    "batch_size = x_.size(0)\n",
    "length = torch.cuda.LongTensor(inputs_len)\n",
    "length_y = torch.cuda.LongTensor(dec_inputs_len)\n",
    "\n",
    "sorted_lengths, sorted_idx = torch.sort(length, descending=True)\n",
    "input_sequence = x_[sorted_idx.cuda()]\n",
    "input_embedding = S2S.embedding(input_sequence)\n",
    "_,reversed_idx = torch.sort(sorted_idx.cuda())\n",
    "packed_input = rnn_utils.pack_padded_sequence(input_embedding, sorted_lengths.tolist(), batch_first=True)\n",
    "\n",
    "encoder_outputs, encoder_hidden_state,encoder_hidden_state_attn = S2S.encoder(packed_input,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_outputs = rnn_utils.pad_packed_sequence(encoder_outputs, batch_first=True)[0]\n",
    "padded_outputs = padded_outputs.contiguous()\n",
    "_,reversed_idx = torch.sort(sorted_idx.cuda())\n",
    "encoder_outputs = padded_outputs[reversed_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input=Variable(torch.cuda.LongTensor(encoder_outputs.size(0))).fill_(sos_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Variable(torch.cuda.LongTensor(target_index[start:end])).transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(encoder_hidden_state.size()) == 1:\n",
    "    decoder_hidden = encoder_hidden_state.unsqueeze(0)\n",
    "else: \n",
    "    decoder_hidden = encoder_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "decoded_words = []\n",
    "decoder_attentions = torch.zeros(encoder_outputs.size(0),dec_max_len, enc_max_len)\n",
    "for i in range(dec_max_len):\n",
    "    #print(decoder_input,decoder_hidden)\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input,decoder_hidden.float(),encoder_outputs.float())\n",
    "    #print(decoder_hidden)\n",
    "    decoder_attentions[:decoder_attention.size(0),i,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    #ni = topi[0]\n",
    "    '''\n",
    "    if ni == eos_idx:\n",
    "        decoded_words.append(_EOS_)\n",
    "        break\n",
    "    else:\n",
    "        decoded_words.append(index2word[ni])\n",
    "    '''\n",
    "    temp = []\n",
    "    if len(topi.size()) == 1:\n",
    "        temp.append(vocab.id2word(topi.cpu().numpy()[0]))\n",
    "    else:\n",
    "        for top in topi.cpu().numpy():\n",
    "            temp.append(vocab.id2word(top[0]))\n",
    "    decoded_words.append(temp)\n",
    "    decoder_input = Variable(topi).cuda()\n",
    "    decoder_hidden = decoder_hidden.squeeze()\n",
    "    if len(decoder_hidden.size()) == 1:\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(0)\n",
    "    else: \n",
    "        decoder_hidden = decoder_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferences = []\n",
    "for decoded_sent in np.array(decoded_words).transpose():\n",
    "    for i,word in enumerate(decoded_sent):\n",
    "        if word == STOP_DECODING:\n",
    "            decoded_sent = decoded_sent[:i]\n",
    "\n",
    "    inferences.append(list(decoded_sent))\n",
    "\n",
    "\n",
    "targets_result = []\n",
    "for inputs in target_index[start:end]:\n",
    "    result = []\n",
    "    for word in inputs:\n",
    "        if word == eos_idx:\n",
    "            break\n",
    "        else:\n",
    "            result.append(vocab.id2word(word))\n",
    "    targets_result.append(result)    \n",
    "\n",
    "inputs_result = []\n",
    "for inputs in input_index[start:end]:\n",
    "    result = []\n",
    "    for word in inputs:\n",
    "        result.append(vocab.id2word(word))\n",
    "    inputs_result.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for source,target,inference, decoder_attention in zip(inputs_result, targets_result, inferences, decoder_attentions):\n",
    "    print('*********************************************************************')\n",
    "    print('source : '+' '.join(source))\n",
    "    \n",
    "    print('*********************************************************************')\n",
    "    print('target : '+' '.join(target))\n",
    "    \n",
    "    print('*********************************************************************')\n",
    "    print('inference : '+' '.join(inference))\n",
    "    \n",
    "    print('*********************************************************************')\n",
    "    scores = rouge.get_scores(' '.join(target), ' '.join(inference))\n",
    "    for score in scores[0]:\n",
    "        print(score +' : '+str(scores[0][score]))\n",
    "\n",
    "    plt.matshow(decoder_attention.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1_recall = []\n",
    "r2_recall = []\n",
    "rl_recall = []\n",
    "\n",
    "r1_f1 = []\n",
    "r2_f1 = []\n",
    "rl_f1 = []\n",
    "\n",
    "error = 0\n",
    "\n",
    "for source,target,inference, decoder_attention in tqdm.tqdm(zip(inputs_result, targets_result, inferences, decoder_attentions)):\n",
    "    try:\n",
    "        scores = rouge.get_scores(' '.join(target), ' '.join(inference))\n",
    "    except ValueError:\n",
    "        error +=1\n",
    "        print('******************************************')\n",
    "        print('target'+' '.join(target))\n",
    "        print('inference'+' '.join(inference))\n",
    "        pass\n",
    "    r1_recall.append(scores[0]['rouge-1']['r'])\n",
    "    r2_recall.append(scores[0]['rouge-2']['r'])\n",
    "    rl_recall.append(scores[0]['rouge-l']['r'])\n",
    "    \n",
    "    r1_f1.append(scores[0]['rouge-1']['f'])\n",
    "    r2_f1.append(scores[0]['rouge-2']['f'])\n",
    "    rl_f1.append(scores[0]['rouge-l']['f'])\n",
    "print('error sentence : ' + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ROUGE-1 recall : ' + str(sum(r1_recall)*100 / len(r1_recall)))\n",
    "print('ROUGE-1 F1 : ' + str(sum(r1_f1)*100 / len(r1_f1)))\n",
    "print('********************************************************************')\n",
    "print('ROUGE-2 recall : ' + str(sum(r2_recall)*100 / len(r2_recall)))\n",
    "print('ROUGE-2 F1 : ' + str(sum(r2_f1)*100 / len(r2_f1)))\n",
    "print('********************************************************************')\n",
    "print('ROUGE-l recall : ' + str(sum(rl_recall)*100 / len(rl_recall)))\n",
    "print('ROUGE-l F1 : ' + str(sum(rl_f1)*100 / len(rl_f1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
